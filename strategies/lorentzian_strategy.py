import pandas as pd
import numpy as np
from typing import Optional, Dict, Any
from datetime import datetime, timezone

from strategies.base_strategy import BaseStrategy
from core.schemas import TradingSignal
from core.enums import SignalType, Timeframe
from utils.logging_config import get_logger

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from ml.lorentzian_indicators import LorentzianIndicators, LorentzianFilters
from ml.enhanced_lorentzian_classifier import EnhancedLorentzianClassifier, create_lorentzian_labels

logger = get_logger(__name__)


class LorentzianStrategy(BaseStrategy):
  """
  –°—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Machine Learning: Lorentzian Classification
  –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É TradingView –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—É
  """

  def __init__(self, config: Dict[str, Any]):
    super().__init__(strategy_name="Lorentzian_Classification")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    self.settings = config.get('strategy_settings', {})

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    self.neighbors_count = self.settings.get('neighbors_count', 8)
    self.max_bars_back = self.settings.get('max_bars_back', 2000)
    self.feature_count = self.settings.get('feature_count', 5)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    self.feature_configs = {
      'f1': {'type': 'RSI', 'paramA': 14, 'paramB': 1},
      'f2': {'type': 'WT', 'paramA': 10, 'paramB': 11},
      'f3': {'type': 'CCI', 'paramA': 20, 'paramB': 1},
      'f4': {'type': 'ADX', 'paramA': 20, 'paramB': 2},
      'f5': {'type': 'RSI', 'paramA': 9, 'paramB': 1}
    }

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    for i in range(1, 6):
      feature_key = f'feature_{i}'
      if feature_key in self.settings:
        self.feature_configs[f'f{i}'] = self.settings[feature_key]

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    self.filter_settings = {
      'use_volatility_filter': self.settings.get('use_volatility_filter', True),
      'use_regime_filter': self.settings.get('use_regime_filter', True),
      'use_adx_filter': self.settings.get('use_adx_filter', False),
      'regime_threshold': self.settings.get('regime_threshold', -0.1),
      'adx_threshold': self.settings.get('adx_threshold', 20)
    }

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
    self.use_dynamic_exits = self.settings.get('use_dynamic_exits', False)
    self.show_bar_predictions = self.settings.get('show_bar_predictions', True)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    self.indicators = LorentzianIndicators()
    self.filters = LorentzianFilters()
    self.classifier = None
    self.is_trained = False

    # # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    # self.training_history = []
    # self.min_history_size = 500  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - –ò–ó–ú–ï–ù–ï–ù–û
    self.training_history = []
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
    self.min_history_size = self.settings.get('min_history_size', 200)  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 500
    self.force_training = self.settings.get('force_training', True)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

    # –ö–µ—à –¥–ª—è –º–æ–¥–µ–ª–µ–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    self.models_cache = {}  # {symbol: classifier}
    self.training_data_cache = {}  # {symbol: (features, labels)}
    self.data_fetcher = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–∑–∂–µ

  def set_data_fetcher(self, data_fetcher):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç data_fetcher –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
    self.data_fetcher = data_fetcher

  async def generate_signal(self, symbol: str, data: pd.DataFrame) -> Optional[TradingSignal]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Lorentzian Classification
    """
    try:
      # # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
      # if len(data) < 50:
      #   return None

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–∏–º–≤–æ–ª–∞
      if len(data) < self.min_history_size and self.data_fetcher:
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}: —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ {len(data)}")
        try:
          # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–æ–ª—å—à–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
          extended_data = await self.data_fetcher.get_historical_candles(
            symbol,
            Timeframe.FIVE_MINUTES,
            limit=2000  # –ó–∞–≥—Ä—É–∂–∞–µ–º 1000 —Å–≤–µ—á–µ–π
          )

          if not extended_data.empty and len(extended_data) > len(data):
            data = extended_data
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å–≤–µ—á–µ–π –¥–ª—è {symbol}")
        except Exception as e:
          logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º—É–º –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
      if len(data) < 100:  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –º–∏–Ω–∏–º—É–º
        logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {len(data)} < 100")
        return None

      # 1. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
      features = self.indicators.calculate_features(
        data,
        self.feature_configs['f1'],
        self.feature_configs['f2'],
        self.feature_configs['f3'],
        self.feature_configs['f4'],
        self.feature_configs['f5']
      )

      if features.empty or len(features) < 10:
        logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {symbol}")
        return None

      # 2. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
      if not self.is_trained:
        if not await self._train_model(data, features):
          return None

      # 3. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      last_features = features.tail(1)

      # 4. –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
      prediction = self.classifier.predict(last_features)[0]
      prediction_proba = self.classifier.predict_proba(last_features)[0]

      # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
      filter_results = self._apply_filters(data)

      # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã - –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–∞
      if not all(filter_results.values()):
        logger.debug(f"–°–∏–≥–Ω–∞–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –¥–ª—è {symbol}: {filter_results}")
        return None

      # 6. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ —Å–∏–≥–Ω–∞–ª
      signal_type = None
      confidence = 0.0

      if prediction == 1:  # BUY
        signal_type = SignalType.BUY
        confidence = prediction_proba[1]
      elif prediction == 2:  # SELL
        signal_type = SignalType.SELL
        confidence = prediction_proba[2]
      else:  # HOLD
        return None

      # 7. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∑–∏—Ü–∏–∏
      current_price = float(data['close'].iloc[-1])
      atr = float(data['atr'].iloc[-1]) if 'atr' in data.columns else current_price * 0.02

      # Stop Loss –∏ Take Profit
      if signal_type == SignalType.BUY:
        stop_loss = current_price - 2 * atr
        take_profit = current_price + 3 * atr
      else:  # SELL
        stop_loss = current_price + 2 * atr
        take_profit = current_price - 3 * atr

      # 8. –°–æ–∑–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª
      signal = TradingSignal(
        symbol=symbol,
        signal_type=signal_type,
        price=current_price,
        confidence=confidence,
        stop_loss=stop_loss,
        take_profit=take_profit,
        strategy_name=self.strategy_name,
        timestamp=datetime.now(timezone.utc),
        metadata={
          'prediction_raw': int(prediction),
          'probabilities': {
            'hold': float(prediction_proba[0]),
            'buy': float(prediction_proba[1]),
            'sell': float(prediction_proba[2])
          },
          'filters_passed': filter_results,
          'features': {
            'f1': float(last_features['f1'].iloc[0]),
            'f2': float(last_features['f2'].iloc[0]),
            'f3': float(last_features['f3'].iloc[0]),
            'f4': float(last_features['f4'].iloc[0]),
            'f5': float(last_features['f5'].iloc[0])
          }
        }
      )

      logger.info(f"üéØ Lorentzian —Å–∏–≥–Ω–∞–ª –¥–ª—è {symbol}: {signal_type.value} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.3f}")

      return signal

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Lorentzian —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è {symbol}: {e}", exc_info=True)
      return None

  async def _train_model(self, data: pd.DataFrame, features: pd.DataFrame) -> bool:
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    try:
      logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Lorentzian –º–æ–¥–µ–ª–∏...")

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 4 –±–∞—Ä–∞ –≤–ø–µ—Ä–µ–¥ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
      labels = create_lorentzian_labels(data, future_bars=4, threshold_percent=0.0)

      # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
      common_index = features.index.intersection(labels.index)
      features_train = features.loc[common_index]
      labels_train = labels.loc[common_index]

      if len(features_train) < self.min_history_size:
        if self.force_training and len(features_train) >= 100:
          logger.warning(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å {len(features_train)} –ø—Ä–∏–º–µ—Ä–∞–º–∏")
        else:
          logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(features_train)} < {self.min_history_size}")
          return False

      # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
      self.classifier = EnhancedLorentzianClassifier(
        k_neighbors=self.neighbors_count,
        max_bars_back=self.max_bars_back,
        feature_count=self.feature_count,
        use_dynamic_exits=self.use_dynamic_exits,
        filters=self.filter_settings
      )

      self.classifier.fit(features_train, labels_train)
      self.is_trained = True

      # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
      train_predictions = self.classifier.predict(features_train.tail(100))
      train_labels = labels_train.tail(100).values
      accuracy = np.mean(train_predictions == train_labels)

      logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 –ø—Ä–∏–º–µ—Ä–∞—Ö: {accuracy:.3f}")

      return True

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
      return False

  def _apply_filters(self, data: pd.DataFrame) -> Dict[str, bool]:
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
    """
    results = {}

    # Volatility Filter
    if self.filter_settings['use_volatility_filter']:
      volatility_pass = self.filters.volatility_filter(data)
      results['volatility'] = bool(volatility_pass.iloc[-1]) if not volatility_pass.empty else True
    else:
      results['volatility'] = True

    # Regime Filter
    if self.filter_settings['use_regime_filter']:
      regime_pass = self.filters.regime_filter(data, self.filter_settings['regime_threshold'])
      results['regime'] = bool(regime_pass.iloc[-1]) if not regime_pass.empty else True
    else:
      results['regime'] = True

    # ADX Filter
    if self.filter_settings['use_adx_filter']:
      adx_pass = self.filters.adx_filter(data, self.filter_settings['adx_threshold'])
      results['adx'] = bool(adx_pass.iloc[-1]) if not adx_pass.empty else True
    else:
      results['adx'] = True

    return results

  def update_model(self, symbol: str, new_data: pd.DataFrame, actual_outcome: int):
    """
    –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
        new_data: DataFrame —Å –Ω–æ–≤—ã–º –±–∞—Ä–æ–º (OHLCV)
        actual_outcome: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (-1=SELL, 0=HOLD, 1=BUY)
    """
    if symbol not in self.models_cache:
      logger.warning(f"–ú–æ–¥–µ–ª—å –¥–ª—è {symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
      return

    try:
      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –±–∞—Ä–∞
      new_features = self.indicators.calculate_features(
        new_data,
        self.feature_configs['f1'],
        self.feature_configs['f2'],
        self.feature_configs['f3'],
        self.feature_configs['f4'],
        self.feature_configs['f5']
      )

      if new_features.empty:
        return

      # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      feature_vector = new_features.iloc[-1].values

      # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å
      classifier = self.models_cache[symbol]
      classifier.incremental_update(feature_vector, actual_outcome, symbol)

      # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à –¥–∞–Ω–Ω—ã—Ö
      if symbol not in self.training_data_cache:
        self.training_data_cache[symbol] = {
          'features': [],
          'labels': [],
          'update_count': 0,
          'last_accuracy': 0.0
        }

      cache = self.training_data_cache[symbol]
      cache['features'].append(feature_vector)
      cache['labels'].append(actual_outcome)
      cache['update_count'] += 1

      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–µ—à–∞
      if len(cache['features']) > self.max_bars_back:
        cache['features'].pop(0)
        cache['labels'].pop(0)

      # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
      if cache['update_count'] % 20 == 0:
        self._evaluate_model_performance(symbol)

      # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ 100 –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
      if cache['update_count'] % 100 == 0 and self.settings.get('save_models', True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"ml_models/lorentzian_{symbol}_{timestamp}.pkl"
        classifier.save_model(model_path)
        logger.info(f"–ú–æ–¥–µ–ª—å {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ—Å–ª–µ {cache['update_count']} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π")

      logger.debug(f"–ú–æ–¥–µ–ª—å {symbol} –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –í—Å–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {cache['update_count']}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è {symbol}: {e}", exc_info=True)

  def _evaluate_model_performance(self, symbol: str):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
    try:
      cache = self.training_data_cache.get(symbol)
      if not cache or len(cache['features']) < 50:
        return

      classifier = self.models_cache.get(symbol)
      if not classifier:
        return

      # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
      test_features = np.array(cache['features'][-50:])
      test_labels = np.array(cache['labels'][-50:])

      # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      feature_names = [f'f{i + 1}' for i in range(self.feature_count)]
      test_df = pd.DataFrame(test_features, columns=feature_names)

      # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      predictions = classifier.predict(test_df)

      # –°—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
      accuracy = np.mean(predictions == test_labels)

      # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
      improvement = accuracy - cache['last_accuracy']
      cache['last_accuracy'] = accuracy

      logger.info(f"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å {symbol}: —Ç–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}, "
                  f"–∏–∑–º–µ–Ω–µ–Ω–∏–µ={improvement:+.3f}")

      # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–ª—å–Ω–æ —É–ø–∞–ª–∞, –º–æ–∂–µ–º –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
      if improvement < -0.1 and len(cache['features']) >= self.min_history_size:
        logger.warning(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {symbol}. "
                       f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {symbol}: {e}")

  def _update_nearest_neighbors(self, symbol: str, new_feature_vector: np.ndarray, new_label: int):
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    –≠–º—É–ª–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≤ TradingView
    """
    if symbol not in self.models_cache:
      return

    classifier = self.models_cache[symbol]

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Ç–æ—á–∫—É –≤ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å
    # –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã EnhancedLorentzianClassifier

    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ - –ø–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    if not hasattr(classifier, 'pending_updates'):
      classifier.pending_updates = []

    classifier.pending_updates.append((new_feature_vector, new_label))

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º predict
    if len(classifier.pending_updates) > 10:
      logger.debug(f"–ù–∞–∫–æ–ø–ª–µ–Ω–æ {len(classifier.pending_updates)} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –¥–ª—è {symbol}")