# rl/train_rl_agent.py

import asyncio
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import matplotlib.pyplot as plt

from config.config_manager import ConfigManager
from core.data_fetcher import DataFetcher
from core.bybit_connector import BybitConnector
from core.enums import Timeframe
from data.database_manager import AdvancedDatabaseManager
from ml.feature_engineering import AdvancedFeatureEngineer
from ml.enhanced_ml_system import EnhancedEnsembleModel
from ml.anomaly_detector import MarketAnomalyDetector
from ml.volatility_system import VolatilityPredictionSystem, ModelType
from core.market_regime_detector import MarketRegimeDetector
from core.risk_manager import AdvancedRiskManager

from rl.environment import BybitTradingEnvironment
from rl.finrl_agent import EnhancedRLAgent
from rl.feature_processor import RLFeatureProcessor
from rl.reward_functions import RiskAdjustedRewardFunction
from rl.data_preprocessor import prepare_data_for_finrl
from stable_baselines3.common.callbacks import BaseCallback

from rl.safe_wrapper import SafeEnvironmentWrapper
from utils.logging_config import get_logger

logger = get_logger(__name__)


class RLTrainer:
  """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""

  def __init__(self, config: Dict[str, Any]):
    self.config_manager = ConfigManager(config_path='../config.json')
    self.config = self.config_manager.load_config()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    self.connector = None
    self.data_fetcher = None
    self.db_manager = None
    self.feature_engineer = None
    self.ml_model = None
    self.anomaly_detector = None
    self.volatility_predictor = None
    self.market_regime_detector = None
    self.risk_manager = None

    # RL –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    self.rl_agent = None
    self.feature_processor = None
    self.environment = None

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    self.training_results = {}

  def _load_config(self, config_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    try:
      with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)
    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
      return {}

  async def initialize_components(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"""
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL...")

    try:
      # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
      self.connector = BybitConnector()
      self.data_fetcher = DataFetcher(self.connector, self.config)
      self.db_manager = AdvancedDatabaseManager()
      self.feature_processor = RLFeatureProcessor(self.config)

      # ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
      self.feature_engineer = AdvancedFeatureEngineer()

      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ML –º–æ–¥–µ–ª—å
      self.ml_model = EnhancedEnsembleModel(
        # feature_columns=[],  # –ë—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        # use_market_regime=True,
        # use_correlation_filter=True
      )

      # –î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π
      self.anomaly_detector = MarketAnomalyDetector(
        contamination=0.1,
        # n_estimators=100
        lookback_periods=100
      )
      logger.info("–î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏")
      # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏, –µ—Å–ª–∏ –æ–±—É—á–∏—Ç—å –Ω–µ —É–¥–∞–µ—Ç—Å—è
      if self.anomaly_detector:
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏...")
        # –î–µ—Ç–µ–∫—Ç–æ—Ä –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

      # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
      self.volatility_predictor = VolatilityPredictionSystem(
        model_type=ModelType.LIGHTGBM,
        # prediction_horizon=[1, 3, 5, 10],
        prediction_horizon=5,
        auto_retrain=True
      )

      # –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–æ–≤ —Ä—ã–Ω–∫–∞
      self.market_regime_detector = MarketRegimeDetector(self.data_fetcher)

      # –†–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä
      self.risk_manager = AdvancedRiskManager(
        # initial_capital=self.rl_config.get('initial_capital', 10000),
        # db_manager=self.db_manager
        db_manager=self.db_manager,
        settings=self.config,  # self.config —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ä–∞–Ω–µ–µ
        data_fetcher=self.data_fetcher,  # self.data_fetcher —É–∂–µ —Å–æ–∑–¥–∞–Ω
        volatility_predictor=None
      )

      # RL Feature Processor
      self.feature_processor = RLFeatureProcessor(
        feature_engineer=self.feature_engineer,
        config=self.config.get('feature_config', {})
      )

      logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}", exc_info=True)
      raise

  # async def load_training_data(self) -> Optional[pd.DataFrame]:
  #   """
  #   –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
  #   –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–º–≤–æ–ª–æ–≤.
  #   """
  #   logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
  #
  #   # --- –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ "—Å—ã—Ä—ã—Ö" –¥–∞–Ω–Ω—ã—Ö ---
  #   symbols = self.config.get('symbols', ['BTCUSDT', 'ETHUSDT'])
  #   timeframe = Timeframe.ONE_HOUR
  #   limit = self.config.get('training_config', {}).get('history_bars', 2000)
  #
  #   raw_data_dict = {}
  #   for symbol in symbols:
  #     data = await self.data_fetcher.get_historical_candles(
  #       symbol=symbol, timeframe=timeframe, limit=limit
  #     )
  #     if data is not None and not data.empty:
  #       raw_data_dict[symbol] = data
  #
  #   if not raw_data_dict:
  #     raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")
  #
  #   # --- –®–∞–≥ 2: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
  #   unaligned_df = prepare_data_for_finrl(raw_data_dict, list(raw_data_dict.keys()))
  #
  #   df_pivot = unaligned_df.pivot(index='date', columns='tic', values='close').dropna()
  #   aligned_df = unaligned_df[unaligned_df.date.isin(df_pivot.index)]
  #
  #   data_with_custom_features = await self._add_technical_indicators(aligned_df)
  #
  #   # --- –®–∞–≥ 3: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ ML –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã ---
  #   logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ ML-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–º–≤–æ–ª–æ–≤...")
  #
  #   tasks = []
  #   # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ 'tic' –∏ –∏—Ç–µ—Ä–∏—Ä—É–µ–º, –ø–æ–ª—É—á–∞—è –∏–º—è –≥—Ä—É–ø–ø—ã (symbol) –∏ —Å–∞–º—É –≥—Ä—É–ø–ø—É (group_df)
  #   for symbol, group_df in data_with_custom_features.groupby('tic'):
  #     tasks.append(self._add_ml_features(group_df, symbol))
  #
  #   # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
  #   results = await asyncio.gather(*tasks, return_exceptions=True)
  #
  #   # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–¥–∏–Ω DataFrame
  #   processed_groups = [res for res in results if isinstance(res, pd.DataFrame)]
  #   if not processed_groups:
  #     raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å ML –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã.")
  #
  #   data_with_ml_features = pd.concat(processed_groups, ignore_index=True)
  #   data_with_ml_features.sort_values(['date', 'tic'], inplace=True)
  #
  #   # --- –®–∞–≥ 4: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö FinRL –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
  #   finrl_ready_df = self._add_finrl_indicators(data_with_ml_features)
  #
  #   if finrl_ready_df is None or finrl_ready_df.empty:
  #     raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ FinRL")
  #
  #   logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(finrl_ready_df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
  #
  #   return finrl_ready_df

  async def load_training_data(self) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    try:
      symbols = self.config.get('symbols', ["BTCUSDT",
                "ETHUSDT",
                "SOLUSDT",
                "XRPUSDT"])
      timeframe = Timeframe.ONE_HOUR

      logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

      # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º
      all_data = {}

      # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π
      max_limit = 1000

      for symbol in symbols:
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")

        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        all_candles = []
        last_timestamp = None

        # –î–µ–ª–∞–µ–º –¥–æ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ 200 —Å–≤–µ—á–µ–π = 2000 —Å–≤–µ—á–µ–π –º–∞–∫—Å–∏–º—É–º
        for i in range(10):
          try:
            if i > 0:
              await asyncio.sleep(0.2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            if hasattr(self.connector, '_make_request'):
              # –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ API Bybit
              params = {
                "symbol": symbol,
                "interval": "60",  # 1 —á–∞—Å
                "limit": 200
              }

              # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –î–û –Ω–µ–µ
              if last_timestamp:
                params["endTime"] = last_timestamp

              result = await self.connector._make_request("GET", "/v5/market/kline", params)

              if result and result.get("result") and result["result"].get("list"):
                klines = result["result"]["list"]

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
                df = pd.DataFrame(klines)
                df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover']

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã
                df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')
                for col in ['open', 'high', 'low', 'close', 'volume', 'turnover']:
                  df[col] = pd.to_numeric(df[col], errors='coerce')

                all_candles.append(df)

                # –û–±–Ω–æ–≤–ª—è–µ–º last_timestamp –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                last_timestamp = int(df['timestamp'].min().timestamp() * 1000) - 1

                logger.info(f"  –ó–∞–ø—Ä–æ—Å {i + 1}: –ø–æ–ª—É—á–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π")
              else:
                logger.warning(f"  –ó–∞–ø—Ä–æ—Å {i + 1}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                break

            else:
              # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥
              data = await self.data_fetcher.get_historical_candles(
                symbol=symbol,
                timeframe=timeframe,
                limit=200,
                use_cache=False
              )

              if data is not None and not data.empty:
                all_candles.append(data)
                logger.info(f"  –ó–∞–ø—Ä–æ—Å {i + 1}: –ø–æ–ª—É—á–µ–Ω–æ {len(data)} —Å–≤–µ—á–µ–π")
              else:
                break

          except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {i + 1} –¥–ª—è {symbol}: {e}")
            break

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if all_candles:
          combined_data = pd.concat(all_candles, ignore_index=True)

          # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ timestamp
          if 'timestamp' in combined_data.columns:
            combined_data = combined_data.drop_duplicates(subset=['timestamp'], keep='first')
            combined_data = combined_data.sort_values('timestamp').reset_index(drop=True)

          logger.info(f"‚úÖ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–ª—è {symbol}: {len(combined_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–≤–µ—á–µ–π")

          # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
          if 'timestamp' not in combined_data.columns and isinstance(combined_data.index, pd.DatetimeIndex):
            combined_data = combined_data.reset_index()
            if 'index' in combined_data.columns:
              combined_data.rename(columns={'index': 'timestamp'}, inplace=True)

          # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
          if 'volume_ratio' not in combined_data.columns:
            combined_data['volume_ratio'] = combined_data['volume'] / combined_data['volume'].rolling(window=20,
                                                                                                      min_periods=1).mean()
            combined_data['volume_ratio'] = combined_data['volume_ratio'].fillna(1.0)

          if 'rsi' not in combined_data.columns:
            delta = combined_data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            combined_data['rsi'] = 100 - (100 / (1 + rs))
            combined_data['rsi'] = combined_data['rsi'].fillna(50)

          if 'sma_20' not in combined_data.columns:
            combined_data['sma_20'] = combined_data['close'].rolling(window=20, min_periods=1).mean()

          # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
          combined_data = combined_data.fillna(method='ffill').fillna(method='bfill').fillna(0)

          # –°–æ—Ö—Ä–∞–Ω—è–µ–º
          all_data[symbol] = combined_data
        else:
          logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}")
        return pd.DataFrame()

      # –î–û–ë–ê–í–ò–¢–¨ –î–ò–ê–ì–ù–û–°–¢–ò–ö–£:
      logger.info("üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
      for symbol, data in all_data.items():
        logger.info(f"  {symbol}: {len(data)} —Å–≤–µ—á–µ–π")
        if 'timestamp' in data.columns:
          logger.info(f"    –ü–µ—Ä–∏–æ–¥: {data['timestamp'].min()} - {data['timestamp'].max()}")

      # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ prepare_data_for_finrl
      df = prepare_data_for_finrl(all_data, list(all_data.keys()))

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
      if df is not None and not df.empty:
        unique_dates = df['date'].nunique()
        total_rows = len(df)
        rows_per_symbol = total_rows // len(symbols)

        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
        logger.info(f"   - –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}")
        logger.info(f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç: {unique_dates}")
        logger.info(f"   - –°—Ç—Ä–æ–∫ –Ω–∞ —Å–∏–º–≤–æ–ª: {rows_per_symbol}")

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        min_required_per_symbol = 800
        if rows_per_symbol < min_required_per_symbol:
          logger.warning(f"‚ö†Ô∏è –î–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ: {rows_per_symbol} < {min_required_per_symbol}")

      return df

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
      return pd.DataFrame()

  async def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
    import pandas_ta as ta

    # RSI
    data['rsi'] = ta.rsi(data['close'], length=14)

    # MACD
    macd = ta.macd(data['close'])
    if macd is not None and not macd.empty:
      data['macd'] = macd.iloc[:, 0]
      data['macd_signal'] = macd.iloc[:, 1]
      data['macd_diff'] = macd.iloc[:, 2]

    # Bollinger Bands
    bb = ta.bbands(data['close'], length=20, std=2)
    if bb is not None and not bb.empty:
      data['bb_lower'] = bb.iloc[:, 0]
      data['bb_middle'] = bb.iloc[:, 1]
      data['bb_upper'] = bb.iloc[:, 2]

    # ATR
    data['atr'] = ta.atr(data['high'], data['low'], data['close'], length=14)

    # ADX
    adx = ta.adx(data['high'], data['low'], data['close'], length=14)
    if adx is not None and not adx.empty:
      data['adx'] = adx.iloc[:, 0]

    # CCI
    data['cci'] = ta.cci(data['high'], data['low'], data['close'], length=20)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    data = data.fillna(method='bfill').fillna(0)

    return data

  async def _add_ml_features(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç ML –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    try:
      # –î–µ—Ç–µ–∫—Ü–∏—è —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
      regime = await self.market_regime_detector.detect_regime(symbol, data)

      # –ò–°–ü–†–ê–í–õ–ï–ù–û: RegimeCharacteristics –Ω–µ –∏–º–µ–µ—Ç .value
      if hasattr(regime, 'name'):
        data['market_regime'] = regime.name
      else:
        data['market_regime'] = str(regime) if regime else 'UNKNOWN'

      # –ß–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞
      regime_mapping = {
        'STRONG_TREND_UP': 4,
        'TREND_UP': 3,
        'RANGE_BOUND': 2,
        'TREND_DOWN': 1,
        'STRONG_TREND_DOWN': 0,
        'UNKNOWN': -1
      }
      data['market_regime_numeric'] = regime_mapping.get(data['market_regime'].iloc[-1], -1)

      # –ü—Ä–æ–≥–Ω–æ–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
      try:
        if hasattr(self.volatility_predictor, 'predict_volatility'):
          vol_pred = self.volatility_predictor.predict_volatility(data)
        elif hasattr(self.volatility_predictor, 'predict_future_volatility'):
          vol_pred = self.volatility_predictor.predict_future_volatility(data)
        else:
          vol_pred = data['close'].pct_change().rolling(20).std().iloc[-1]

        if isinstance(vol_pred, dict):
          data['predicted_volatility'] = vol_pred.get('volatility', 0) or vol_pred.get('predictions', {}).get(1, 0)
        else:
          data['predicted_volatility'] = float(vol_pred) if vol_pred else 0.0
      except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏: {e}")
        data['predicted_volatility'] = 0.0

      # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±–∏—Ä–∞–µ–º await, —Ç–∞–∫ –∫–∞–∫ detect_anomalies —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π
      anomaly_reports = self.anomaly_detector.detect_anomalies(data, symbol)
      if anomaly_reports:

        anomaly_score = max(report.severity for report in anomaly_reports)
      else:
        anomaly_score = 0.0
      data['anomaly_score'] = anomaly_score

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è ML –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
      # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
      data['market_regime'] = 'UNKNOWN'
      data['market_regime_numeric'] = -1
      data['predicted_volatility'] = 0.0
      data['anomaly_score'] = 0.0

    return data

  def _add_finrl_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ FinRL –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if 'tic' not in df.columns:
      raise ValueError("DataFrame –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'tic'")

    # –°–ø–∏—Å–æ–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å
    indicators_config = {
      'rsi': {'default': 50.0, 'required': True},
      'macd': {'default': 0.0, 'required': True},
      'macd_signal': {'default': 0.0, 'required': True},
      'macd_diff': {'default': 0.0, 'required': True},
      'cci': {'default': 0.0, 'required': True},
      'adx': {'default': 25.0, 'required': True},
      'atr': {'default': 0.0, 'required': True}
    }

    result_dfs = []

    for tic in df['tic'].unique():
      tic_df = df[df['tic'] == tic].copy()

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
      price_cols = ['open', 'high', 'low', 'close', 'volume']
      for col in price_cols:
        if col not in tic_df.columns:
          raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ {col} –¥–ª—è {tic}")

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ
        tic_df[col] = pd.to_numeric(tic_df[col], errors='coerce')

      # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
      for indicator, config in indicators_config.items():
        if indicator not in tic_df.columns:
          tic_df[indicator] = config['default']
        else:
          # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
          tic_df[indicator] = pd.to_numeric(tic_df[indicator], errors='coerce').fillna(config['default'])

      # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ—Ç NaN
      tic_df = tic_df.fillna(method='ffill').fillna(method='bfill')

      # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å NaN, –∑–∞–ø–æ–ª–Ω—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
      for col in tic_df.columns:
        if tic_df[col].isna().any():
          if col in indicators_config:
            tic_df[col] = tic_df[col].fillna(indicators_config[col]['default'])
          else:
            tic_df[col] = tic_df[col].fillna(0)

      result_dfs.append(tic_df)

    if not result_dfs:
      raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    final_df = pd.concat(result_dfs, ignore_index=True)
    final_df = final_df.sort_values(['date', 'tic']).reset_index(drop=True)

    logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã. –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞: {final_df.shape}")
    logger.info(f"–ö–æ–ª–æ–Ω–∫–∏: {final_df.columns.tolist()}")

    return final_df

  async def create_environment(self, df: pd.DataFrame) -> SafeEnvironmentWrapper:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–æ—Ä–≥–æ–≤—É—é —Å—Ä–µ–¥—É"""
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ä–µ–¥—ã...")

    unique_dates = df['date'].nunique()
    unique_symbols = df['tic'].nunique()

    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã: {unique_dates} –¥–Ω–µ–π, {unique_symbols} —Å–∏–º–≤–æ–ª–æ–≤")

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è FinRL
    min_days_required = 800  # –î–ª—è —ç–ø–∏–∑–æ–¥–æ–≤ –ø–æ 800 –¥–Ω–µ–π
    if unique_dates < min_days_required:
      logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–Ω–µ–π: {unique_dates} < {min_days_required}")

    if len(df) < 800:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
      logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} —Å—Ç—Ä–æ–∫. –î–æ–ø–æ–ª–Ω—è–µ–º...")
      # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ª–∏–±–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ —á—Ç–æ –µ—Å—Ç—å

    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
    base_columns = ['date', 'tic', 'open', 'high', 'low', 'close', 'volume', 'timestamp', 'turnover']

    # –ù–∞—Ö–æ–¥–∏–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    tech_indicators = []
    for col in df.columns:
      if col not in base_columns and df[col].dtype in ['float64', 'int64', 'float32']:
        tech_indicators.append(col)

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {len(tech_indicators)}")
    logger.info(f"–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {tech_indicators[:10]}...")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10

    # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    logger.info(f"–í—Ö–æ–¥–Ω–æ–π DataFrame shape: {df.shape}")
    logger.info(f"Columns: {df.columns.tolist()}")

    if 'tic' in df.columns:
      logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ tickers: {df['tic'].unique()}")

    if 'date' in df.columns:
      logger.info(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    logger.info(f"DataFrame dtypes:\n{df.dtypes}")
    logger.info(f"–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:\n{df.head()}")

    # –û—Ç–ª–∞–¥–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    debug_dataframe_structure(df, "Before environment creation")

    # –ö–†–ò–¢–ò–ß–ù–û: –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è FinRL
    if 'tic' not in df.columns or 'date' not in df.columns:
      raise ValueError("DataFrame –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'tic' –∏ 'date'")

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å - —ç—Ç–æ RangeIndex, –∞ –Ω–µ –¥–∞—Ç—ã
    if not isinstance(df.index, pd.RangeIndex):
      df = df.reset_index(drop=True)

    # –ü—Ä–æ–≤–µ—Ä–∏–º –∏ –∏—Å–ø—Ä–∞–≤–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    numeric_columns = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_columns:
      if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
        if df[col].isna().any():
          logger.warning(f"Found NaN in {col}, filling with forward fill")
          df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

    # –í–ê–ñ–ù–û: FinRL —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –≤—ã—Ä–æ–≤–Ω–µ–Ω—ã
    df = df.sort_values(['date', 'tic']).reset_index(drop=True)

    # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
    reward_function = RiskAdjustedRewardFunction(
      risk_manager=self.risk_manager,
      config=self.config.get('reward_config', {})
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã
    env_config = {
      'hmax': 100,
      'initial_amount': self.config.get('initial_capital', 10000),
      'transaction_cost_pct': 0.001,
      'reward_scaling': 1e-4,
      'buy_cost_pct': 0.001,
      'sell_cost_pct': 0.001,
      'tech_indicator_list': tech_indicators
    }

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —Å—Ä–µ–¥—ã
    debug_dataframe_structure(df, "Final check before environment")

    try:
      # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
      environment = BybitTradingEnvironment(
        df=df,
        data_fetcher=self.data_fetcher,
        market_regime_detector=self.market_regime_detector,
        risk_manager=self.risk_manager,
        shadow_trading_manager=None,
        feature_engineer=self.feature_engineer,
        initial_balance=env_config['initial_amount'],
        commission_rate=env_config['transaction_cost_pct'],
        leverage=self.config.get('leverage', 10),
        max_positions=self.config.get('portfolio_config', {}).get('max_positions', 10),
        config=env_config
      )

      logger.info("‚úÖ –¢–æ—Ä–≥–æ–≤–∞—è —Å—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
      # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
      logger.info(f"State space: {environment.state_space}")
      logger.info(f"Action space: {environment.action_space}")
      logger.info(f"Stock dim: {environment.stock_dim}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã: {e}")
      logger.error(f"DataFrame info:\n{df.info()}")
      raise

    from rl.safe_wrapper import SafeEnvironmentWrapper
    safe_environment = SafeEnvironmentWrapper(environment)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è

    # environment.reward_function = reward_function
    safe_environment.reward_function = reward_function

    return safe_environment

  async def train_agent(self, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """–û–±—É—á–∞–µ—Ç RL –∞–≥–µ–Ω—Ç–∞"""
    logger.info("=" * 50)
    logger.info("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø RL –ê–ì–ï–ù–¢–ê")
    logger.info("=" * 50)

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    train_env = await self.create_environment(train_df)
    test_env = await self.create_environment(test_df)

    # –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É:
    logger.info(f"Test environment info: obs_space={test_env.observation_space.shape}, "
                f"action_space={test_env.action_space.shape}")

    # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ eval_env –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
    if hasattr(test_env, 'df'):
      logger.info(f"Test env data shape: {test_env.df.shape}")

    # –°–æ–∑–¥–∞–µ–º RL –∞–≥–µ–Ω—Ç–∞
    self.rl_agent = EnhancedRLAgent(
      environment=train_env,
      ml_model=self.ml_model,
      anomaly_detector=self.anomaly_detector,
      volatility_predictor=self.volatility_predictor,
      algorithm=self.config.get('algorithm', 'PPO'),
      config=self.config
    )



    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_config = self.config.get('training_config', {})
    total_timesteps = training_config.get('total_timesteps', 100000)
    eval_freq = training_config.get('eval_frequency', 10000)
    save_freq = training_config.get('save_frequency', 20000)



    # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    class TrainingCallback(BaseCallback):
      """
      –ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
      """

      def __init__(self, trainer, verbose=0):
        super(TrainingCallback, self).__init__(verbose)
        self.trainer = trainer
        self.episode_rewards = []
        self.episode_lengths = []
        self.n_episodes = 0

      def _on_step(self) -> bool:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Å—Ä–µ–¥—ã
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
        if hasattr(self.locals, 'rewards') and len(self.locals['rewards']) > 0:
          last_reward = self.locals['rewards'][-1]
          if last_reward != 0:
            logger.info(f"üéØ –ù–µ–Ω—É–ª–µ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {last_reward}")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if self.num_timesteps % 100 == 0:
          # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏–∑ —Å—Ä–µ–¥—ã
          if hasattr(self.model, '_last_obs') and self.model._last_obs is not None:
            obs = self.model._last_obs
            if isinstance(obs, np.ndarray) and len(obs) > 0:
              # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç (–±–∞–ª–∞–Ω—Å)
              balance = obs[0][0] if obs.ndim > 1 else obs[0]
              logger.debug(f"–®–∞–≥ {self.num_timesteps}: –ë–∞–ª–∞–Ω—Å = {balance:.2f}")

              # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
              if balance <= 0 or balance > 1e6:
                logger.warning(f"–ê–Ω–æ–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –Ω–∞ —à–∞–≥–µ {self.num_timesteps}: {balance}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞
        if self.locals.get('dones', [False])[0]:
          self.n_episodes += 1

          # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º —ç–ø–∏–∑–æ–¥–µ
          info = self.locals.get('infos', [{}])[0]
          episode_reward = info.get('episode', {}).get('r', 0)
          episode_length = info.get('episode', {}).get('l', 0)

          self.episode_rewards.append(episode_reward)
          self.episode_lengths.append(episode_length)

          # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
          if self.n_episodes % 10 == 0:
            avg_reward = np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0
            avg_length = np.mean(self.episode_lengths[-10:]) if self.episode_lengths else 0

            logger.info(f"\n{'=' * 50}")
            logger.info(f"–≠–ø–∏–∑–æ–¥: {self.n_episodes}")
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10): {avg_reward:.2f}")
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {avg_length:.0f}")
            logger.info(f"–í—Å–µ–≥–æ —à–∞–≥–æ–≤: {self.num_timesteps}")
            logger.info(f"{'=' * 50}\n")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ info_buffer –µ—Å–ª–∏ –µ—Å—Ç—å
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
          for ep_info in self.model.ep_info_buffer:
            if 'r' in ep_info and 'l' in ep_info:
              self.episode_rewards.append(ep_info['r'])
              self.episode_lengths.append(ep_info['l'])
              self.n_episodes += 1

        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.num_timesteps % 1000 == 0:
          logger.info(
            f"–®–∞–≥ {self.num_timesteps}/{self.trainer.config.get('training_config', {}).get('total_timesteps', 100000)}")
          if self.episode_rewards:
            avg_reward = np.mean(self.episode_rewards[-10:])
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤): {avg_reward:.2f}")

        # # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5000 —à–∞–≥–æ–≤
        # if self.num_timesteps % 5000 == 0 and self.num_timesteps > 0:
        #   logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {self.num_timesteps} —à–∞–≥–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")

        if len(self.episode_rewards) > 20:
          recent_avg = np.mean(self.episode_rewards[-10:])
          older_avg = np.mean(self.episode_rewards[-20:-10])

          if recent_avg > older_avg * 1.1:  # –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 10%
            logger.info(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ! –°—Ç–∞—Ä–æ–µ: {older_avg:.2f}, –ù–æ–≤–æ–µ: {recent_avg:.2f}")


        return True  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ

      def _on_rollout_end(self) -> None:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ rollout
        """
        pass

      def _on_training_end(self) -> None:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

      def __call__(self, locals_dict, globals_dict):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if 'episode_rewards' in locals_dict:
          self.episode_rewards.extend(locals_dict['episode_rewards'])
        if 'episode_lengths' in locals_dict:
          self.episode_lengths.extend(locals_dict['episode_lengths'])

        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if locals_dict['self'].num_timesteps % 1000 == 0:
          logger.info(f"–®–∞–≥ {locals_dict['self'].num_timesteps}/{total_timesteps}")
          if self.episode_rewards:
            avg_reward = np.mean(self.episode_rewards[-10:])
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤): {avg_reward:.2f}")

        return True

    callback = TrainingCallback(self, verbose=1)

    # –û–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    logger.info(f"–û–±—É—á–µ–Ω–∏–µ {self.config.get('algorithm', 'PPO')} –Ω–∞ {total_timesteps} —à–∞–≥–æ–≤...")

    self.rl_agent.train(
      total_timesteps=total_timesteps,
      callback=callback,
      log_interval=100,
      eval_env=test_env,
      eval_freq=eval_freq,
      n_eval_episodes=5,
      save_freq=save_freq
    )

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    logger.info("\n" + "=" * 60)
    logger.info("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø")
    logger.info("=" * 60)

    if hasattr(callback, 'episode_rewards') and callback.episode_rewards:
      total_episodes = len(callback.episode_rewards)
      avg_reward = np.mean(callback.episode_rewards)
      std_reward = np.std(callback.episode_rewards)
      max_reward = np.max(callback.episode_rewards)
      min_reward = np.min(callback.episode_rewards)

      logger.info(f"–í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {total_episodes}")
      logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.2f} ¬± {std_reward:.2f}")
      logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {max_reward:.2f}")
      logger.info(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {min_reward:.2f}")

      # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç—Ä–µ–Ω–¥
      if total_episodes > 20:
        early_avg = np.mean(callback.episode_rewards[:10])
        late_avg = np.mean(callback.episode_rewards[-10:])
        improvement = ((late_avg - early_avg) / abs(early_avg)) * 100 if early_avg != 0 else 0

        logger.info(f"\n–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        logger.info(f"–ü–µ—Ä–≤—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {early_avg:.2f}")
        logger.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {late_avg:.2f}")
        logger.info(f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ: {improvement:+.1f}%")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    self.training_results = {
      'algorithm': self.config.get('algorithm', 'PPO'),
      'total_timesteps': total_timesteps,
      'episode_rewards': callback.episode_rewards,
      'episode_lengths': callback.episode_lengths,
      'training_time': datetime.now().isoformat(),
      'symbols': self.config.get('symbols', []),
      'final_stats': self.rl_agent.get_training_stats()
    }

    logger.info("=" * 50)
    logger.info("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.training_results['final_stats']}")
    logger.info("=" * 50)

  async def evaluate_agent(self, test_df: pd.DataFrame):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞"""
    logger.info("–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞...")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É
    test_env = await self.create_environment(test_df)

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–∑–≤—Ä–∞—Ç reset()
    obs, info = test_env.reset()  # reset –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂
    done = False
    truncated = False

    rewards = []
    actions = []
    portfolio_values = []

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞
    initial_balance = test_env.initial_amount if hasattr(test_env, 'initial_amount') else 10000
    portfolio_values.append(initial_balance)

    while not done and not truncated:
      # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç –∞–≥–µ–Ω—Ç–∞
      action, _ = self.rl_agent.predict(obs, deterministic=True)  # obs —É–∂–µ numpy array

      # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
      obs, reward, done, truncated, info = test_env.step(action)  # 5 –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
      rewards.append(reward)
      actions.append(action)

      # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –±–∞–ª–∞–Ω—Å –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
      if isinstance(obs, np.ndarray) and len(obs) > 0:
        current_balance = obs[0]  # –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è - —ç—Ç–æ –±–∞–ª–∞–Ω—Å
        portfolio_values.append(current_balance)

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if len(portfolio_values) > 1:
      total_return = (portfolio_values[-1] - portfolio_values[0]) / portfolio_values[0]
    else:
      total_return = 0

    sharpe_ratio = self._calculate_sharpe_ratio(rewards)
    max_drawdown = self._calculate_max_drawdown(portfolio_values)
    win_rate = sum(1 for r in rewards if r > 0) / len(rewards) if rewards else 0

    evaluation_results = {
      'total_return': total_return,
      'total_return_pct': total_return * 100,
      'sharpe_ratio': sharpe_ratio,
      'max_drawdown': max_drawdown,
      'win_rate': win_rate,
      'total_trades': len([a for a in actions if np.any(a != 0)]),  # –ù–µ —Å—á–∏—Ç–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
      'final_portfolio_value': portfolio_values[-1] if portfolio_values else initial_balance,
      'total_rewards': sum(rewards)
    }

    self.training_results['evaluation'] = evaluation_results

    logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    logger.info(f"  –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return * 100:.2f}%")
    logger.info(f"  Sharpe Ratio: {sharpe_ratio:.2f}")
    logger.info(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {max_drawdown * 100:.2f}%")
    logger.info(f"  Win Rate: {win_rate * 100:.2f}%")
    logger.info(f"  –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {evaluation_results['total_trades']}")

    return evaluation_results

  def _calculate_sharpe_ratio(self, returns: List[float]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Sharpe Ratio"""
    if not returns or len(returns) < 2:
      return 0

    returns_array = np.array(returns)
    if np.std(returns_array) == 0:
      return 0

    # Annualized Sharpe
    sharpe = np.mean(returns_array) / np.std(returns_array) * np.sqrt(252)
    return sharpe

  def _calculate_max_drawdown(self, values: List[float]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ—Å–∞–¥–∫—É"""
    if not values:
      return 0

    peak = values[0]
    max_dd = 0

    for value in values:
      if value > peak:
        peak = value
      drawdown = (peak - value) / peak
      max_dd = max(max_dd, drawdown)

    return max_dd

  def save_results(self):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_dir = Path("rl/training_results")
    results_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"{self.config.get('algorithm', 'PPO')}_{timestamp}"
    self.rl_agent.save_model(model_name)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file = results_dir / f"training_results_{timestamp}.json"
    with open(results_file, 'w') as f:
      json.dump(self.training_results, f, indent=2, default=str)

    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    self._create_visualizations(results_dir, timestamp)

    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_dir}")
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {model_name}")

  def _create_visualizations(self, results_dir: Path, timestamp: str):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    try:
      plt.style.use('seaborn-v0_8-darkgrid')
      fig, axes = plt.subplots(2, 2, figsize=(15, 10))

      # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
      if self.training_results.get('episode_rewards'):
        ax = axes[0, 0]
        rewards = self.training_results['episode_rewards']
        ax.plot(rewards, alpha=0.3, label='Episode Rewards')

        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        if len(rewards) > 10:
          ma = pd.Series(rewards).rolling(10).mean()
          ax.plot(ma, label='MA(10)', linewidth=2)

        ax.set_title('Episode Rewards During Training')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.legend()

      # –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —ç–ø–∏–∑–æ–¥–æ–≤
      if self.training_results.get('episode_lengths'):
        ax = axes[0, 1]
        lengths = self.training_results['episode_lengths']
        ax.plot(lengths, alpha=0.5)
        ax.set_title('Episode Lengths')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length')

      # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
      if 'evaluation' in self.training_results:
        ax = axes[1, 0]
        eval_data = self.training_results['evaluation']

        metrics = ['total_return_pct', 'sharpe_ratio', 'win_rate']
        values = [
          eval_data.get('total_return_pct', 0),
          eval_data.get('sharpe_ratio', 0) * 10,  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
          eval_data.get('win_rate', 0) * 100
        ]
        labels = ['Return %', 'Sharpe x10', 'Win Rate %']

        bars = ax.bar(labels, values)
        ax.set_title('Performance Metrics')
        ax.set_ylabel('Value')

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, values):
          height = bar.get_height()
          ax.text(bar.get_x() + bar.get_width() / 2., height,
                  f'{value:.1f}',
                  ha='center', va='bottom')

      # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
      ax = axes[1, 1]
      ax.axis('off')

      info_text = f"""
            Model Information:

            Algorithm: {self.config.get('algorithm', 'PPO')}
            Total Timesteps: {self.training_results.get('total_timesteps', 0):,}
            Training Time: {timestamp}

            Symbols: {', '.join(self.config.get('symbols', []))}

            Final Stats:
            {json.dumps(self.training_results.get('final_stats', {}), indent=2)}
            """

      ax.text(0.1, 0.9, info_text, transform=ax.transAxes,
              fontsize=10, verticalalignment='top', fontfamily='monospace')

      plt.tight_layout()
      plt.savefig(results_dir / f"training_results_{timestamp}.png", dpi=300, bbox_inches='tight')
      plt.close()

      logger.info("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")


async def main_training():
  """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
  logger.info("=" * 80)
  logger.info("–ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø RL –ê–ì–ï–ù–¢–ê")
  logger.info("=" * 80)

  # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
  import os
  os.makedirs('results', exist_ok=True)
  os.makedirs('rl/models', exist_ok=True)
  os.makedirs('rl/models/logs', exist_ok=True)
  os.makedirs('rl/models/best_model', exist_ok=True)
  os.makedirs('rl/models/eval_logs', exist_ok=True)
  logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞")
  trainer = None
  # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º —Ç—Ä–µ–π–Ω–µ—Ä–∞.
  # –ü—É—Ç—å '../config.json' —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ config
  # –Ω–∞ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ, —á–µ–º –ø–∞–ø–∫–∞ rl.
  # –ï—Å–ª–∏ config.json –≤ –∫–æ—Ä–Ω–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ path='../config.json'
  try:
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø—É—Ç—å: ../ –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–∞ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –≤–≤–µ—Ä—Ö" –æ—Ç –ø–∞–ø–∫–∏ rl
    config_manager = ConfigManager(config_path='../config.json')
    config = config_manager.load_config()
    logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
  except Exception as e:
    logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    return

  # 2. –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä RLTrainer, –ü–ï–†–ï–î–ê–í–ê–Ø –µ–º—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥.
  trainer = RLTrainer(config)

  # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É —Ä–∞–±–æ—Ç—ã
  try:
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    await trainer.initialize_components()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = await trainer.load_training_data()

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º —Ä–∞–±–æ—Ç—É
    if df is None or df.empty:
      logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
      return

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
    unique_dates = df['date'].nunique()
    unique_symbols = df['tic'].nunique()
    total_rows = len(df)

    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö:")
    logger.info(f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç: {unique_dates}")
    logger.info(f"   - –°–∏–º–≤–æ–ª–æ–≤: {unique_symbols}")
    logger.info(f"   - –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}")

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è FinRL
    min_required = 1000 * unique_symbols
    if total_rows < min_required:
      logger.error(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {total_rows} < {min_required}")
      logger.error("   FinRL —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 1000 —Ç–æ—á–µ–∫ –Ω–∞ —Å–∏–º–≤–æ–ª")
      return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_size = int(len(df) * 0.8)
    train_df = df[:train_size]
    test_df = df[train_size:]

    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã: train={len(train_df)}, test={len(test_df)}")

    # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    await trainer.train_agent(train_df, test_df)

    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    await trainer.evaluate_agent(test_df)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.save_results()

    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

  except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
    raise  # –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
  finally:
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    if trainer and hasattr(trainer, 'connector') and trainer.connector:
      await trainer.connector.close()
    if trainer and hasattr(trainer, 'data_fetcher') and hasattr(trainer.data_fetcher, 'connector'):
      await trainer.data_fetcher.connector.close()

def validate_finrl_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç DataFrame –¥–ª—è FinRL
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    required_columns = ['date', 'tic', 'open', 'high', 'low', 'close', 'volume']

    for col in required_columns:
      if col not in df.columns:
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –∫–∞–∂–¥—É—é –¥–∞—Ç—É
    # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è FinRL!
    date_tic_combinations = df.groupby(['date', 'tic']).size()
    dates = df['date'].unique()
    tics = df['tic'].unique()

    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –¥–∞—Ç–∞-—Å–∏–º–≤–æ–ª
    full_index = pd.MultiIndex.from_product([dates, tics], names=['date', 'tic'])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    missing_combinations = set(full_index) - set(date_tic_combinations.index)

    if missing_combinations:
      logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(missing_combinations)} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –¥–∞—Ç–∞-—Å–∏–º–≤–æ–ª")

      # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
      for date, tic in missing_combinations:
        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –∑–∞–ø–∏—Å—å –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        tic_data = df[df['tic'] == tic]
        if len(tic_data) > 0:
          # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∏–∑–≤–µ—Å—Ç–Ω—É—é —Ü–µ–Ω—É
          last_known = tic_data[tic_data['date'] < date].iloc[-1] if len(tic_data[tic_data['date'] < date]) > 0 else \
          tic_data.iloc[0]

          new_row = last_known.copy()
          new_row['date'] = date
          new_row['volume'] = 0  # –ù—É–ª–µ–≤–æ–π –æ–±—ä–µ–º –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–Ω–µ–π

          df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ —Å–∏–º–≤–æ–ª—É
    df = df.sort_values(['date', 'tic']).reset_index(drop=True)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —á–∏—Å–ª–æ–≤—ã–µ
    numeric_cols = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_cols:
      df[col] = pd.to_numeric(df[col], errors='coerce').fillna(method='ffill').fillna(0)

    return df


def debug_dataframe_structure(df: pd.DataFrame, stage: str = ""):
  """–û—Ç–ª–∞–¥–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DataFrame –¥–ª—è FinRL"""
  logger.info(f"\n{'=' * 50}")
  logger.info(f"DEBUG DataFrame Structure - {stage}")
  logger.info(f"{'=' * 50}")
  logger.info(f"Shape: {df.shape}")
  logger.info(f"Columns: {df.columns.tolist()}")
  logger.info(f"Index: {df.index.name} - {type(df.index)}")
  logger.info(f"Dtypes:\n{df.dtypes}")

  if 'tic' in df.columns:
    logger.info(f"Unique tickers: {df['tic'].unique()}")
    logger.info(f"Ticker counts:\n{df['tic'].value_counts()}")

  if 'date' in df.columns:
    logger.info(f"Date range: {df['date'].min()} to {df['date'].max()}")
    logger.info(f"Unique dates: {df['date'].nunique()}")

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
  if 'date' in df.columns and 'tic' in df.columns:
    duplicates = df.duplicated(subset=['date', 'tic'])
    if duplicates.any():
      logger.warning(f"Found {duplicates.sum()} duplicate date-tic combinations!")

  # –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫
  logger.info(f"First 5 rows:\n{df.head()}")
  logger.info(f"{'=' * 50}\n")


if __name__ == "__main__":
  asyncio.run(main_training())