# rl/train_rl_agent.py

import asyncio
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns

from config.config_manager import ConfigManager
from core.data_fetcher import DataFetcher
from core.bybit_connector import BybitConnector
from core.enums import Timeframe
from data.database_manager import AdvancedDatabaseManager
from ml.feature_engineering import AdvancedFeatureEngineer
from ml.enhanced_ml_system import EnhancedEnsembleModel
from ml.anomaly_detector import MarketAnomalyDetector
from ml.volatility_system import VolatilityPredictionSystem, ModelType
from core.market_regime_detector import MarketRegimeDetector
from core.risk_manager import AdvancedRiskManager

from rl.environment import BybitTradingEnvironment
from rl.finrl_agent import EnhancedRLAgent
from rl.feature_processor import RLFeatureProcessor
from rl.reward_functions import RiskAdjustedRewardFunction
from rl.data_preprocessor import prepare_data_for_finrl

from utils.logging_config import get_logger

logger = get_logger(__name__)


class RLTrainer:
  """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""

  def __init__(self, config: Dict[str, Any]):
    self.config_manager = ConfigManager(config_path='../config.json')
    self.config = self.config_manager.load_config()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    self.connector = None
    self.data_fetcher = None
    self.db_manager = None
    self.feature_engineer = None
    self.ml_model = None
    self.anomaly_detector = None
    self.volatility_predictor = None
    self.market_regime_detector = None
    self.risk_manager = None

    # RL –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    self.rl_agent = None
    self.feature_processor = None
    self.environment = None

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    self.training_results = {}

  def _load_config(self, config_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    try:
      with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)
    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
      return {}

  async def initialize_components(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"""
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RL...")

    try:
      # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
      self.connector = BybitConnector()
      self.data_fetcher = DataFetcher(self.connector, self.config)
      self.db_manager = AdvancedDatabaseManager()
      self.feature_processor = RLFeatureProcessor(self.config)

      # ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
      self.feature_engineer = AdvancedFeatureEngineer()

      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ML –º–æ–¥–µ–ª—å
      self.ml_model = EnhancedEnsembleModel(
        # feature_columns=[],  # –ë—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        # use_market_regime=True,
        # use_correlation_filter=True
      )

      # –î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π
      self.anomaly_detector = MarketAnomalyDetector(
        contamination=0.1,
        # n_estimators=100
        lookback_periods=100
      )

      # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
      self.volatility_predictor = VolatilityPredictionSystem(
        model_type=ModelType.LIGHTGBM,
        # prediction_horizon=[1, 3, 5, 10],
        prediction_horizon=5,
        auto_retrain=True
      )

      # –î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–æ–≤ —Ä—ã–Ω–∫–∞
      self.market_regime_detector = MarketRegimeDetector(self.data_fetcher)

      # –†–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä
      self.risk_manager = AdvancedRiskManager(
        # initial_capital=self.rl_config.get('initial_capital', 10000),
        # db_manager=self.db_manager
        db_manager=self.db_manager,
        settings=self.config,  # self.config —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ä–∞–Ω–µ–µ
        data_fetcher=self.data_fetcher,  # self.data_fetcher —É–∂–µ —Å–æ–∑–¥–∞–Ω
        volatility_predictor=None
      )

      # RL Feature Processor
      self.feature_processor = RLFeatureProcessor(
        feature_engineer=self.feature_engineer,
        config=self.config.get('feature_config', {})
      )

      logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}", exc_info=True)
      raise

  # async def load_training_data(self) -> Optional[pd.DataFrame]:
  #   """
  #   –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
  #   –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–º–≤–æ–ª–æ–≤.
  #   """
  #   logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
  #
  #   # --- –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ "—Å—ã—Ä—ã—Ö" –¥–∞–Ω–Ω—ã—Ö ---
  #   symbols = self.config.get('symbols', ['BTCUSDT', 'ETHUSDT'])
  #   timeframe = Timeframe.ONE_HOUR
  #   limit = self.config.get('training_config', {}).get('history_bars', 2000)
  #
  #   raw_data_dict = {}
  #   for symbol in symbols:
  #     data = await self.data_fetcher.get_historical_candles(
  #       symbol=symbol, timeframe=timeframe, limit=limit
  #     )
  #     if data is not None and not data.empty:
  #       raw_data_dict[symbol] = data
  #
  #   if not raw_data_dict:
  #     raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")
  #
  #   # --- –®–∞–≥ 2: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
  #   unaligned_df = prepare_data_for_finrl(raw_data_dict, list(raw_data_dict.keys()))
  #
  #   df_pivot = unaligned_df.pivot(index='date', columns='tic', values='close').dropna()
  #   aligned_df = unaligned_df[unaligned_df.date.isin(df_pivot.index)]
  #
  #   data_with_custom_features = await self._add_technical_indicators(aligned_df)
  #
  #   # --- –®–∞–≥ 3: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ ML –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã ---
  #   logger.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ ML-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Å–∏–º–≤–æ–ª–æ–≤...")
  #
  #   tasks = []
  #   # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ 'tic' –∏ –∏—Ç–µ—Ä–∏—Ä—É–µ–º, –ø–æ–ª—É—á–∞—è –∏–º—è –≥—Ä—É–ø–ø—ã (symbol) –∏ —Å–∞–º—É –≥—Ä—É–ø–ø—É (group_df)
  #   for symbol, group_df in data_with_custom_features.groupby('tic'):
  #     tasks.append(self._add_ml_features(group_df, symbol))
  #
  #   # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
  #   results = await asyncio.gather(*tasks, return_exceptions=True)
  #
  #   # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–¥–∏–Ω DataFrame
  #   processed_groups = [res for res in results if isinstance(res, pd.DataFrame)]
  #   if not processed_groups:
  #     raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å ML –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã.")
  #
  #   data_with_ml_features = pd.concat(processed_groups, ignore_index=True)
  #   data_with_ml_features.sort_values(['date', 'tic'], inplace=True)
  #
  #   # --- –®–∞–≥ 4: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö FinRL –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
  #   finrl_ready_df = self._add_finrl_indicators(data_with_ml_features)
  #
  #   if finrl_ready_df is None or finrl_ready_df.empty:
  #     raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ FinRL")
  #
  #   logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(finrl_ready_df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
  #
  #   return finrl_ready_df

  async def load_training_data(self) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")

    symbols = self.config.get('symbols', ['BTCUSDT', 'ETHUSDT'])
    timeframe = Timeframe.ONE_HOUR
    limit = self.config.get('training_config', {}).get('history_bars', 2000)

    all_data = {}

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in symbols:
      logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}...")

      try:
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        data = await self.data_fetcher.get_historical_candles(
          symbol=symbol,
          timeframe=timeframe,
          limit=limit
        )

        if data is not None and not data.empty:
          # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
          data = await self._add_technical_indicators(data)

          # –î–æ–±–∞–≤–ª—è–µ–º ML –ø—Ä–∏–∑–Ω–∞–∫–∏
          data = await self._add_ml_features(data, symbol)

          all_data[symbol] = data
          logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –±–∞—Ä–æ–≤ –¥–ª—è {symbol}")
        else:
          logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")

      except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
        continue

    if not all_data:
      raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç FinRL
    finrl_df = prepare_data_for_finrl(all_data, list(all_data.keys()))

    # –î–û–ë–ê–í–õ–Ø–ï–ú –ü–†–û–í–ï–†–ö–£ –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•
    logger.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ prepare_data_for_finrl:")
    logger.info(f"–ö–æ–ª–æ–Ω–∫–∏: {finrl_df.columns.tolist()}")
    logger.info(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n{finrl_df.dtypes}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    for col in ['open', 'high', 'low', 'close', 'volume']:
      if col not in finrl_df.columns:
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ
      sample_value = finrl_df[col].iloc[0] if len(finrl_df) > 0 else None
      logger.info(f"–ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è {col}: {sample_value}, —Ç–∏–ø: {type(sample_value)}")

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ FinRL
    finrl_df = self._add_finrl_indicators(finrl_df)

    logger.info(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(finrl_df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    return finrl_df

  async def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
    import pandas_ta as ta

    # RSI
    data['rsi'] = ta.rsi(data['close'], length=14)

    # MACD
    macd = ta.macd(data['close'])
    if macd is not None and not macd.empty:
      data['macd'] = macd.iloc[:, 0]
      data['macd_signal'] = macd.iloc[:, 1]
      data['macd_diff'] = macd.iloc[:, 2]

    # Bollinger Bands
    bb = ta.bbands(data['close'], length=20, std=2)
    if bb is not None and not bb.empty:
      data['bb_lower'] = bb.iloc[:, 0]
      data['bb_middle'] = bb.iloc[:, 1]
      data['bb_upper'] = bb.iloc[:, 2]

    # ATR
    data['atr'] = ta.atr(data['high'], data['low'], data['close'], length=14)

    # ADX
    adx = ta.adx(data['high'], data['low'], data['close'], length=14)
    if adx is not None and not adx.empty:
      data['adx'] = adx.iloc[:, 0]

    # CCI
    data['cci'] = ta.cci(data['high'], data['low'], data['close'], length=20)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    data = data.fillna(method='bfill').fillna(0)

    return data

  async def _add_ml_features(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç ML –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    try:
      # –î–µ—Ç–µ–∫—Ü–∏—è —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
      regime = await self.market_regime_detector.detect_regime(symbol, data)

      # –ò–°–ü–†–ê–í–õ–ï–ù–û: RegimeCharacteristics –Ω–µ –∏–º–µ–µ—Ç .value
      if hasattr(regime, 'name'):
        data['market_regime'] = regime.name
      else:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –∏–ª–∏ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        data['market_regime'] = str(regime) if regime else 'UNKNOWN'

      # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —á–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ:
      regime_mapping = {
        'STRONG_TREND_UP': 4,
        'TREND_UP': 3,
        'RANGE_BOUND': 2,
        'TREND_DOWN': 1,
        'STRONG_TREND_DOWN': 0,
        'UNKNOWN': -1
      }
      data['market_regime_numeric'] = regime_mapping.get(data['market_regime'].iloc[-1], -1)

      # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
      try:
        if hasattr(self.volatility_predictor, 'predict_volatility'):
          vol_pred = await self.volatility_predictor.predict_volatility(symbol, data)
        elif hasattr(self.volatility_predictor, 'predict_future_volatility'):
          vol_pred = self.volatility_predictor.predict_future_volatility(data)
        else:
          # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –º–µ—Ç–æ–¥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç
          vol_pred = data['close'].pct_change().rolling(20).std().iloc[-1]

        if isinstance(vol_pred, dict):
          data['predicted_volatility'] = vol_pred.get('volatility', 0) or vol_pred.get('predictions', {}).get(1, 0)
        else:
          data['predicted_volatility'] = float(vol_pred) if vol_pred else 0.0
      except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏: {e}")
        data['predicted_volatility'] = 0.0

      # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
      anomaly_reports = await self.anomaly_detector.detect_anomalies(data, symbol)
      if anomaly_reports:
        # –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω–∫–∏ –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é "—Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å" –∞–Ω–æ–º–∞–ª–∏–∏
        anomaly_score = max(report.severity for report in anomaly_reports)
      data['anomaly_score'] = anomaly_score

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è ML –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
      # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
      data['market_regime'] = 'UNKNOWN'
      data['market_regime_numeric'] = -1
      data['predicted_volatility'] = 0.0
      data['anomaly_score'] = 0.0

    return data

  def _add_finrl_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ FinRL –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # Verify input structure
    if 'tic' not in df.columns:
      raise ValueError("Input DataFrame must contain 'tic' column")

    result_dfs = []

    for tic in df['tic'].unique():
      tic_df = df[df['tic'] == tic].copy()

      # Validate price data
      price_cols = ['open', 'high', 'low', 'close']
      if not all(col in tic_df.columns for col in price_cols):
        raise ValueError(f"Missing price columns for {tic}")

      # Ensure numeric values
      for col in price_cols:
        tic_df[col] = pd.to_numeric(tic_df[col], errors='coerce')
        if tic_df[col].isna().any():
          raise ValueError(f"Non-numeric values in {col} for {tic}")

      # Add indicators with proper defaults
      indicators = {
        'rsi': 50.0,
        'macd': 0.0,
        'macd_signal': 0.0,
        'macd_diff': 0.0,
        'cci': 0.0,
        'adx': 25.0,
        'atr': 0.0
      }

      for indicator, default in indicators.items():
        if indicator not in tic_df.columns:
          tic_df[indicator] = default
        tic_df[indicator] = pd.to_numeric(tic_df[indicator], errors='coerce').fillna(default)

      result_dfs.append(tic_df)

    if not result_dfs:
      raise ValueError("No valid data after processing")

    return pd.concat(result_dfs).sort_values(['date', 'tic']).reset_index(drop=True)

  async def create_environment(self, df: pd.DataFrame) -> BybitTradingEnvironment:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–æ—Ä–≥–æ–≤—É—é —Å—Ä–µ–¥—É"""
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ä–µ–¥—ã...")

    # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
    reward_function = RiskAdjustedRewardFunction(
        risk_manager=self.risk_manager,
        config=self.config.get('reward_config', {})
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ä–µ–¥—ã - –¥–æ–±–∞–≤–ª—è–µ–º reward_scaling
    env_config = {
        'hmax': 100,
        'initial_amount': self.config.get('initial_capital', 10000),
        'transaction_cost_pct': 0.001,
        'reward_scaling': 1e-4,  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ –∑–¥–µ—Å—å
        'buy_cost_pct': 0.001,
        'sell_cost_pct': 0.001
    }

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
    environment = BybitTradingEnvironment(
        df=df,
        data_fetcher=self.data_fetcher,
        market_regime_detector=self.market_regime_detector,
        risk_manager=self.risk_manager,
        shadow_trading_manager=None,
        feature_engineer=self.feature_engineer,
        initial_balance=env_config['initial_amount'],
        commission_rate=env_config['transaction_cost_pct'],
        leverage=self.config.get('leverage', 10),
        max_positions=self.config.get('portfolio_config', {}).get('max_positions', 10),
        config=env_config  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
    )

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
    environment.reward_function = reward_function

    logger.info("‚úÖ –¢–æ—Ä–≥–æ–≤–∞—è —Å—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞")

    return environment

  async def train_agent(self, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """–û–±—É—á–∞–µ—Ç RL –∞–≥–µ–Ω—Ç–∞"""
    logger.info("=" * 50)
    logger.info("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø RL –ê–ì–ï–ù–¢–ê")
    logger.info("=" * 50)

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    train_env = await self.create_environment(train_df)
    test_env = await self.create_environment(test_df)

    # –°–æ–∑–¥–∞–µ–º RL –∞–≥–µ–Ω—Ç–∞
    self.rl_agent = EnhancedRLAgent(
      environment=train_env,
      ml_model=self.ml_model,
      anomaly_detector=self.anomaly_detector,
      volatility_predictor=self.volatility_predictor,
      algorithm=self.config.get('algorithm', 'PPO'),
      config=self.config
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_config = self.config.get('training_config', {})
    total_timesteps = training_config.get('total_timesteps', 100000)
    eval_freq = training_config.get('eval_frequency', 10000)
    save_freq = training_config.get('save_frequency', 20000)

    # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    class TrainingCallback:
      def __init__(self, trainer):
        self.trainer = trainer
        self.episode_rewards = []
        self.episode_lengths = []

      def __call__(self, locals_dict, globals_dict):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if 'episode_rewards' in locals_dict:
          self.episode_rewards.extend(locals_dict['episode_rewards'])
        if 'episode_lengths' in locals_dict:
          self.episode_lengths.extend(locals_dict['episode_lengths'])

        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if locals_dict['self'].num_timesteps % 1000 == 0:
          logger.info(f"–®–∞–≥ {locals_dict['self'].num_timesteps}/{total_timesteps}")
          if self.episode_rewards:
            avg_reward = np.mean(self.episode_rewards[-10:])
            logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤): {avg_reward:.2f}")

        return True

    callback = TrainingCallback(self)

    # –û–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    logger.info(f"–û–±—É—á–µ–Ω–∏–µ {self.config.get('algorithm', 'PPO')} –Ω–∞ {total_timesteps} —à–∞–≥–æ–≤...")

    self.rl_agent.train(
      total_timesteps=total_timesteps,
      callback=callback,
      log_interval=100,
      eval_env=test_env,
      eval_freq=eval_freq,
      n_eval_episodes=5,
      save_freq=save_freq
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    self.training_results = {
      'algorithm': self.config.get('algorithm', 'PPO'),
      'total_timesteps': total_timesteps,
      'episode_rewards': callback.episode_rewards,
      'episode_lengths': callback.episode_lengths,
      'training_time': datetime.now().isoformat(),
      'symbols': self.config.get('symbols', []),
      'final_stats': self.rl_agent.get_training_stats()
    }

    logger.info("=" * 50)
    logger.info("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    logger.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.training_results['final_stats']}")
    logger.info("=" * 50)

  async def evaluate_agent(self, test_df: pd.DataFrame):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞"""
    logger.info("–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞...")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ä–µ–¥—É
    test_env = await self.create_environment(test_df)

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    obs = test_env.reset()
    done = False

    rewards = []
    actions = []
    portfolio_values = []

    while not done:
      # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç –∞–≥–µ–Ω—Ç–∞
      action, _ = self.rl_agent.predict(obs, deterministic=True)

      # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
      obs, reward, done, _, info = test_env.step(action)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
      rewards.append(reward)
      actions.append(action)
      portfolio_values.append(test_env.amount)

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    total_return = (portfolio_values[-1] - portfolio_values[0]) / portfolio_values[0]
    sharpe_ratio = self._calculate_sharpe_ratio(rewards)
    max_drawdown = self._calculate_max_drawdown(portfolio_values)
    win_rate = sum(1 for r in rewards if r > 0) / len(rewards) if rewards else 0

    evaluation_results = {
      'total_return': total_return,
      'total_return_pct': total_return * 100,
      'sharpe_ratio': sharpe_ratio,
      'max_drawdown': max_drawdown,
      'win_rate': win_rate,
      'total_trades': len([a for a in actions if a != 1]),  # –ù–µ —Å—á–∏—Ç–∞–µ–º HOLD
      'final_portfolio_value': portfolio_values[-1],
      'total_rewards': sum(rewards)
    }

    self.training_results['evaluation'] = evaluation_results

    logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    logger.info(f"  –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return * 100:.2f}%")
    logger.info(f"  Sharpe Ratio: {sharpe_ratio:.2f}")
    logger.info(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {max_drawdown * 100:.2f}%")
    logger.info(f"  Win Rate: {win_rate * 100:.2f}%")
    logger.info(f"  –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {evaluation_results['total_trades']}")

    return evaluation_results

  def _calculate_sharpe_ratio(self, returns: List[float]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Sharpe Ratio"""
    if not returns or len(returns) < 2:
      return 0

    returns_array = np.array(returns)
    if np.std(returns_array) == 0:
      return 0

    # Annualized Sharpe
    sharpe = np.mean(returns_array) / np.std(returns_array) * np.sqrt(252)
    return sharpe

  def _calculate_max_drawdown(self, values: List[float]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ—Å–∞–¥–∫—É"""
    if not values:
      return 0

    peak = values[0]
    max_dd = 0

    for value in values:
      if value > peak:
        peak = value
      drawdown = (peak - value) / peak
      max_dd = max(max_dd, drawdown)

    return max_dd

  def save_results(self):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_dir = Path("rl/training_results")
    results_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"{self.config.get('algorithm', 'PPO')}_{timestamp}"
    self.rl_agent.save_model(model_name)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file = results_dir / f"training_results_{timestamp}.json"
    with open(results_file, 'w') as f:
      json.dump(self.training_results, f, indent=2, default=str)

    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    self._create_visualizations(results_dir, timestamp)

    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_dir}")
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {model_name}")

  def _create_visualizations(self, results_dir: Path, timestamp: str):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    try:
      plt.style.use('seaborn-v0_8-darkgrid')
      fig, axes = plt.subplots(2, 2, figsize=(15, 10))

      # –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–≥—Ä–∞–¥ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
      if self.training_results.get('episode_rewards'):
        ax = axes[0, 0]
        rewards = self.training_results['episode_rewards']
        ax.plot(rewards, alpha=0.3, label='Episode Rewards')

        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        if len(rewards) > 10:
          ma = pd.Series(rewards).rolling(10).mean()
          ax.plot(ma, label='MA(10)', linewidth=2)

        ax.set_title('Episode Rewards During Training')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.legend()

      # –ì—Ä–∞—Ñ–∏–∫ –¥–ª–∏–Ω—ã —ç–ø–∏–∑–æ–¥–æ–≤
      if self.training_results.get('episode_lengths'):
        ax = axes[0, 1]
        lengths = self.training_results['episode_lengths']
        ax.plot(lengths, alpha=0.5)
        ax.set_title('Episode Lengths')
        ax.set_xlabel('Episode')
        ax.set_ylabel('Length')

      # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
      if 'evaluation' in self.training_results:
        ax = axes[1, 0]
        eval_data = self.training_results['evaluation']

        metrics = ['total_return_pct', 'sharpe_ratio', 'win_rate']
        values = [
          eval_data.get('total_return_pct', 0),
          eval_data.get('sharpe_ratio', 0) * 10,  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
          eval_data.get('win_rate', 0) * 100
        ]
        labels = ['Return %', 'Sharpe x10', 'Win Rate %']

        bars = ax.bar(labels, values)
        ax.set_title('Performance Metrics')
        ax.set_ylabel('Value')

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, values):
          height = bar.get_height()
          ax.text(bar.get_x() + bar.get_width() / 2., height,
                  f'{value:.1f}',
                  ha='center', va='bottom')

      # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
      ax = axes[1, 1]
      ax.axis('off')

      info_text = f"""
            Model Information:

            Algorithm: {self.config.get('algorithm', 'PPO')}
            Total Timesteps: {self.training_results.get('total_timesteps', 0):,}
            Training Time: {timestamp}

            Symbols: {', '.join(self.config.get('symbols', []))}

            Final Stats:
            {json.dumps(self.training_results.get('final_stats', {}), indent=2)}
            """

      ax.text(0.1, 0.9, info_text, transform=ax.transAxes,
              fontsize=10, verticalalignment='top', fontfamily='monospace')

      plt.tight_layout()
      plt.savefig(results_dir / f"training_results_{timestamp}.png", dpi=300, bbox_inches='tight')
      plt.close()

      logger.info("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")


async def main_training():
  """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏."""
  logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞")
  trainer = None
  # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º —Ç—Ä–µ–π–Ω–µ—Ä–∞.
  # –ü—É—Ç—å '../config.json' —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ config
  # –Ω–∞ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ, —á–µ–º –ø–∞–ø–∫–∞ rl.
  # –ï—Å–ª–∏ config.json –≤ –∫–æ—Ä–Ω–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ path='../config.json'
  try:
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø—É—Ç—å: ../ –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–∞ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –≤–≤–µ—Ä—Ö" –æ—Ç –ø–∞–ø–∫–∏ rl
    config_manager = ConfigManager(config_path='../config.json')
    config = config_manager.load_config()
    logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
  except Exception as e:
    logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    return

  # 2. –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä RLTrainer, –ü–ï–†–ï–î–ê–í–ê–Ø –µ–º—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥.
  trainer = RLTrainer(config)

  # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É —Ä–∞–±–æ—Ç—ã
  try:
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    await trainer.initialize_components()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = await trainer.load_training_data()

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º —Ä–∞–±–æ—Ç—É
    if df is None or df.empty:
      logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
      return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    train_size = int(len(df) * 0.8)
    train_df = df[:train_size]
    test_df = df[train_size:]

    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã: train={len(train_df)}, test={len(test_df)}")

    # –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    await trainer.train_agent(train_df, test_df)

    # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    await trainer.evaluate_agent(test_df)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.save_results()

    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

  except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
    raise  # –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
  finally:
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    if trainer and hasattr(trainer, 'connector') and trainer.connector:
      await trainer.connector.close()
    if trainer and hasattr(trainer, 'data_fetcher') and hasattr(trainer.data_fetcher, 'connector'):
      await trainer.data_fetcher.connector.close()


if __name__ == "__main__":
  asyncio.run(main_training())