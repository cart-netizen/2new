import asyncio
import shutil

import joblib
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, Tuple, List
from datetime import datetime, timezone
from ml.wavetrend_3d import WaveTrend3D

from strategies.base_strategy import BaseStrategy
from core.schemas import TradingSignal
from core.enums import SignalType, Timeframe
from utils.logging_config import get_logger

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from ml.lorentzian_indicators import LorentzianIndicators, LorentzianFilters
from ml.enhanced_lorentzian_classifier import EnhancedLorentzianClassifier, create_lorentzian_labels
import pickle
import os
from functools import lru_cache
from pathlib import Path  # –î–æ–±–∞–≤—å —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç
import joblib
import pickle
import os
from functools import lru_cache

logger = get_logger(__name__)


class LorentzianStrategy(BaseStrategy):
  """
  –°—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Machine Learning: Lorentzian Classification
  –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É TradingView –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—É
  """

  def __init__(self, config: Dict[str, Any]):
    super().__init__(strategy_name="Lorentzian_Classification")

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    self.settings = config.get('strategy_settings', {})

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    self.neighbors_count = self.settings.get('neighbors_count', 12)
    self.max_bars_back = self.settings.get('max_bars_back', 5000)
    self.feature_count = self.settings.get('feature_count', 5)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    self.feature_configs = {
      'f1': {'type': 'RSI', 'paramA': 7, 'paramB': 1},
      'f2': {'type': 'WT', 'paramA': 5, 'paramB': 7},
      'f3': {'type': 'CCI', 'paramA': 10, 'paramB': 1},
      'f4': {'type': 'ADX', 'paramA': 10, 'paramB': 2},
      'f5': {'type': 'RSI', 'paramA': 5, 'paramB': 1}
    }

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    for i in range(1, 6):
      feature_key = f'feature_{i}'
      if feature_key in self.settings:
        self.feature_configs[f'f{i}'] = self.settings[feature_key]

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    self.filter_settings = {
      'use_volatility_filter': self.settings.get('use_volatility_filter', True),
      'use_regime_filter': self.settings.get('use_regime_filter', True),
      'use_adx_filter': self.settings.get('use_adx_filter', False),
      'regime_threshold': self.settings.get('regime_threshold', -0.1),
      'adx_threshold': self.settings.get('adx_threshold', 20)
    }

    self.use_wavetrend_3d = self.settings.get('use_wavetrend_3d', True)
    self.wavetrend_3d_config = self.settings.get('wavetrend_3d', {})

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º WaveTrend 3D –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
    if self.use_wavetrend_3d:
      self.wavetrend_3d = WaveTrend3D(self.wavetrend_3d_config)
      logger.info("‚úÖ WaveTrend 3D –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ Lorentzian —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")
    else:
      self.wavetrend_3d = None
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
    self.use_dynamic_exits = self.settings.get('use_dynamic_exits', False)
    self.show_bar_predictions = self.settings.get('show_bar_predictions', True)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    self.indicators = LorentzianIndicators()
    self.filters = LorentzianFilters()
    self.classifier = None
    self.is_trained = False

    # # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    # self.min_history_size = 500  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

    # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è - –ò–ó–ú–ï–ù–ï–ù–û
    self.training_history = {}
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
    self.min_history_size = self.settings.get('min_history_size', 5000)  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 500
    self.min_initial_training_size = 5000
    self.force_training = self.settings.get('force_training', False)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

    # –ö–µ—à –¥–ª—è –º–æ–¥–µ–ª–µ–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    self.models_cache = {}  # {symbol: classifier}
    self.training_data_cache = {}  # {symbol: (features, labels)}
    self.data_fetcher = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–∑–∂–µ
    self.performance_optimization = config.get('advanced_settings', {}).get('performance_optimization', {})
    self.use_numba = self.performance_optimization.get('use_numba', True)
    self.cache_predictions = self.performance_optimization.get('cache_predictions', True)
    self.cache_ttl_seconds = self.performance_optimization.get('cache_ttl_seconds', 60)
    self.min_trading_signals_ratio = self.settings.get('min_trading_signals_ratio', 0.15)
    self.quality_threshold = self.settings.get('quality_threshold', 0.55)
    self.trading_accuracy_threshold = self.settings.get('trading_accuracy_threshold', 0.5)
    self.auto_retrain_on_poor_quality = self.settings.get('auto_retrain_on_poor_quality', True)
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    # self.models_dir = "ml_models/lorentzian"
    # os.makedirs(self.models_dir, exist_ok=True)

    self.models_dir = Path("ml_models/lorentzian")
    self.models_dir.mkdir(parents=True, exist_ok=True)

    self._load_existing_models()

  def _load_existing_models(self):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    try:
      # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
      if not self.models_dir.exists():
        logger.info("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º...")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        return

      # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π
      model_files = list(self.models_dir.glob("lorentzian_*.pkl"))

      if not model_files:
        logger.info("–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
        return

      loaded_count = 0
      for model_file in model_files:
        try:
          # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∏–º–≤–æ–ª –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
          # –§–æ—Ä–º–∞—Ç: lorentzian_BTCUSDT_20240804_123456.pkl –∏–ª–∏ lorentzian_BTCUSDT_latest.pkl
          filename = model_file.stem  # –£–±–∏—Ä–∞–µ–º .pkl
          parts = filename.split("_")

          if len(parts) >= 2:
            symbol = parts[1]  # BTCUSDT

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol} –∏–∑ {model_file.name}")

            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            classifier = EnhancedLorentzianClassifier(
              k_neighbors=self.neighbors_count,
              max_bars_back=self.max_bars_back,
              feature_count=self.feature_count,
              use_dynamic_exits=self.use_dynamic_exits,
              filters=self.filter_settings
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
            if classifier.load_model(str(model_file)):
              self.models_cache[symbol] = classifier
              loaded_count += 1
              logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è {symbol} –∏–∑ {model_file.name}")
            else:
              logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ {model_file.name}")

        except Exception as e:
          logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ {model_file}: {e}")
          continue

      if loaded_count > 0:
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
      else:
        logger.info("–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")

  def _save_model(self, symbol: str, classifier):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–º–≤–æ–ª–∞"""
    try:
      timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
      model_path = self.models_dir / f"lorentzian_{symbol}_{timestamp}.pkl"

      # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
      old_models = list(self.models_dir.glob(f"lorentzian_{symbol}_*.pkl"))
      if old_models:
        old_models.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        for old_model in old_models[2:]:
          old_model.unlink()
          logger.debug(f"–£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å: {old_model.name}")

      # # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
      # with open(model_path, 'wb') as f:
      #   joblib.dump(classifier, f)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
      classifier.save_model(str(model_path))

      # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–º–µ–Ω—è–µ–º —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Å—Ç–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ ---
      # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–∞–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –≤ Windows.
      latest_path = self.models_dir / f"lorentzian_{symbol}_latest.pkl"
      try:
          shutil.copy(model_path, latest_path)
          logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path.name} –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –∫–æ–ø–∏—è 'latest'")
      except Exception as copy_error:
          logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–ø–∏—é 'latest' –¥–ª—è –º–æ–¥–µ–ª–∏ {symbol}: {copy_error}")
      # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---

      return True

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {symbol}: {e}")
      return False

  def set_data_fetcher(self, data_fetcher):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç data_fetcher –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
    self.data_fetcher = data_fetcher

  async def generate_signal(self, symbol: str, data: pd.DataFrame) -> Optional[TradingSignal]:
      """
      –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –£–ñ–ï –û–ë–£–ß–ï–ù–ù–û–ô Lorentzian Classification
      """

      # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≥–æ—Ç–æ–≤–∞ –ª–∏ –º–æ–¥–µ–ª—å. –ï—Å–ª–∏ –Ω–µ—Ç - —Å–∏–≥–Ω–∞–ª –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º.
      if symbol not in self.models_cache or not self.models_cache[symbol].is_fitted:
          logger.warning(f"‚è≥ {symbol}: –ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞.")
          # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ preload_training_data, –µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ
          # await self.preload_training_data(symbol)
          return None

      if len(data) < 100:
          logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {len(data)} < 100")
          return None

      # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å
      self.classifier = self.models_cache[symbol]

      # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ü–û–°–õ–ï–î–ù–ò–• –¥–∞–Ω–Ω—ã—Ö
      features = self.indicators.calculate_features(
          data,
          self.feature_configs['f1'], self.feature_configs['f2'], self.feature_configs['f3'],
          self.feature_configs['f4'], self.feature_configs['f5']
      )

      if features.empty:
          logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö {symbol}")
          return None

      # 3. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      last_features = features.tail(1)

      # 4. –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
      prediction = self.classifier.predict(last_features)[0]
      prediction_proba = self.classifier.predict_proba(last_features)[0]

      # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
      filter_results = self._apply_filters(data, symbol)

      # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã - –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–∞
      if not all(filter_results.values()):
        logger.debug(f"–°–∏–≥–Ω–∞–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –¥–ª—è {symbol}: {filter_results}")
        return None

      # 6. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ —Å–∏–≥–Ω–∞–ª
      signal_type = None
      confidence = 0.0

      # 6a. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∏–≥–Ω–∞–ª–∞
      if prediction == 1:  # BUY
        signal_type = SignalType.BUY
        confidence = prediction_proba[1]
      elif prediction == 2:  # SELL
        signal_type = SignalType.SELL
        confidence = prediction_proba[2]
      else:  # HOLD
        return None

      # 6b. –ü—Ä–∏–º–µ–Ω—è–µ–º WaveTrend 3D –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
      if self.use_wavetrend_3d and 'wavetrend_direction' in filter_results:
        wt_direction = filter_results['wavetrend_direction']
        wt_confidence = filter_results['wavetrend_confidence']

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ —Å–∏–≥–Ω–∞–ª–∞
        if (signal_type == SignalType.BUY and wt_direction > 0) or (
            signal_type == SignalType.SELL and wt_direction < 0):
          logger.info(f"‚úÖ WaveTrend 3D –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç Lorentzian –¥–ª—è {symbol}")
          confidence = min(0.95, confidence + wt_confidence * 0.15)
        elif (signal_type == SignalType.BUY and wt_direction < 0) or (
            signal_type == SignalType.SELL and wt_direction > 0):
          logger.warning(f"‚ö†Ô∏è WaveTrend 3D –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç Lorentzian –¥–ª—è {symbol}")
          if wt_confidence > 0.7:  # –°–∏–ª—å–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
            return None
          else:
            confidence = max(0.3, confidence - wt_confidence * 0.2)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        wavetrend_result = self.wavetrend_3d.calculate(data, symbol)
        if wavetrend_result and wavetrend_result.get('divergences'):
          divergences = wavetrend_result['divergences']
          if divergences.get('bullish') or divergences.get('bearish'):
            divergence_type = 'bullish_divergence' if divergences.get('bullish') else 'bearish_divergence'
            logger.info(f"üéØ WaveTrend –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: {divergence_type}")


      # 7. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∑–∏—Ü–∏–∏
      current_price = float(data['close'].iloc[-1])
      atr = float(data['atr'].iloc[-1]) if 'atr' in data.columns else current_price * 0.02

      # Stop Loss –∏ Take Profit
      if signal_type == SignalType.BUY:
        stop_loss = current_price - 2 * atr
        take_profit = current_price + 3 * atr
      else:  # SELL
        stop_loss = current_price + 2 * atr
        take_profit = current_price - 3 * atr

      # 8. –°–æ–∑–¥–∞–µ–º —Å–∏–≥–Ω–∞–ª
      signal = TradingSignal(
        symbol=symbol,
        signal_type=signal_type,
        price=current_price,
        confidence=confidence,
        stop_loss=stop_loss,
        take_profit=take_profit,
        strategy_name=self.strategy_name,
        timestamp=datetime.now(timezone.utc),
        metadata={
          'prediction_raw': int(prediction),
          'probabilities': {
            'hold': float(prediction_proba[0]),
            'buy': float(prediction_proba[1]),
            'sell': float(prediction_proba[2])
          },
          'filters_passed': filter_results,
          'wavetrend_3d': {
            'used': self.use_wavetrend_3d,
            'direction': filter_results.get('wavetrend_direction', 0),
            'confidence': filter_results.get('wavetrend_confidence', 0),
            'passed': filter_results.get('wavetrend_3d', True)
          },
          'features': {
            'f1': float(last_features['f1'].iloc[0]),
            'f2': float(last_features['f2'].iloc[0]),
            'f3': float(last_features['f3'].iloc[0]),
            'f4': float(last_features['f4'].iloc[0]),
            'f5': float(last_features['f5'].iloc[0])
          }
        }
      )

      logger.info(f"üéØ Lorentzian —Å–∏–≥–Ω–∞–ª –¥–ª—è {symbol}: {signal_type.value} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence:.3f}")
      if len(self.training_history) > 0 and (len(self.training_history) % 10 == 0):
        self.log_accumulation_summary()

      return signal



  def analyze_signal_alignment(self, lorentzian_signal: int, wavetrend_data: Dict) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–æ–≤ Lorentzian –∏ WaveTrend 3D

    Returns:
        Dict —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    """
    wt_direction = wavetrend_data.get('direction', 0)
    wt_confidence = wavetrend_data.get('confidence', 0.5)

    # –ü–æ–ª–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ
    if (lorentzian_signal == 1 and wt_direction > 0) or \
        (lorentzian_signal == 2 and wt_direction < 0):
      return {
        'aligned': True,
        'strength': 'strong',
        'confidence_multiplier': 1.0 + wt_confidence * 0.3,
        'recommendation': 'proceed'
      }

    # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
    elif (lorentzian_signal == 1 and wt_direction < 0) or \
        (lorentzian_signal == 2 and wt_direction > 0):
      return {
        'aligned': False,
        'strength': 'conflict',
        'confidence_multiplier': 1.0 - wt_confidence * 0.4,
        'recommendation': 'caution' if wt_confidence < 0.7 else 'skip'
      }

    # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π WaveTrend
    else:
      return {
        'aligned': None,
        'strength': 'neutral',
        'confidence_multiplier': 1.0,
        'recommendation': 'proceed_normal'
      }

  # async def _train_model(self, data: pd.DataFrame, features: pd.DataFrame) -> bool:
  #   """
  #   –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
  #   """
  #   try:
  #     logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Lorentzian –º–æ–¥–µ–ª–∏...")
  #
  #     # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 4 –±–∞—Ä–∞ –≤–ø–µ—Ä–µ–¥ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
  #     labels = create_lorentzian_labels(data, future_bars=4, threshold_percent=0.0)
  #
  #     # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
  #     common_index = features.index.intersection(labels.index)
  #     features_train = features.loc[common_index]
  #     labels_train = labels.loc[common_index]
  #
  #     if len(features_train) < self.min_history_size:
  #       if self.force_training and len(features_train) >= 100:
  #         logger.warning(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å {len(features_train)} –ø—Ä–∏–º–µ—Ä–∞–º–∏")
  #       else:
  #         logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(features_train)} < {self.min_history_size}")
  #         return False
  #
  #     # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
  #     self.classifier = EnhancedLorentzianClassifier(
  #       k_neighbors=self.neighbors_count,
  #       max_bars_back=self.max_bars_back,
  #       feature_count=self.feature_count,
  #       use_dynamic_exits=self.use_dynamic_exits,
  #       filters=self.filter_settings
  #     )
  #
  #     self.classifier.fit(features_train, labels_train)
  #     self.is_trained = True
  #
  #     # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
  #     train_predictions = self.classifier.predict(features_train.tail(300))
  #     train_labels = labels_train.tail(300).values
  #     accuracy = np.mean(train_predictions == train_labels)
  #
  #     logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 300 –ø—Ä–∏–º–µ—Ä–∞—Ö: {accuracy:.3f}")
  #
  #     return True
  #
  #   except Exception as e:
  #     logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
  #     return False

  async def _train_model(self, symbol: str, data: pd.DataFrame, features: pd.DataFrame) -> bool:
      """
      –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º
      """
      try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        if symbol in self.models_cache and self.models_cache[symbol].is_fitted:
          logger.info(f"‚úÖ {symbol}: –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é")
          return True

        logger.info(f"üéØ {symbol}: –Ω–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")

        # –£–ü–†–û–©–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ - —Å—Ä–∞–∑—É –æ–±—É—á–∞–µ–º
        if len(data) >= self.min_initial_training_size and len(features) >= self.min_initial_training_size:
          logger.info(f"üöÄ {symbol}: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(data)}>={self.min_initial_training_size}), "
                      f"–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∏ —Å—Ä–∞–∑—É –æ–±—É—á–∞–µ–º!")

          # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
          training_data = data.tail(self.min_initial_training_size).copy()
          training_features = features.tail(self.min_initial_training_size).copy()

          return await self._immediate_training(symbol, training_data, training_features)

        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ - –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º
        logger.info(f"‚è≥ {symbol}: –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ({len(data)}<{self.min_initial_training_size}), –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º...")
        ready_for_training = self._accumulate_training_data(symbol, data, features)

        if not ready_for_training:
          logger.info(f"‚è≥ {symbol}: –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –∂–¥–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö...")
          return False

        logger.info(f"‚úÖ {symbol}: –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ!")

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        combined_data, combined_features = self._get_accumulated_data(symbol)

        if combined_data.empty or combined_features.empty:
          logger.error(f"‚ùå {symbol}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
          return False

        return await self._immediate_training(symbol, combined_data, combined_features)

      except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {symbol}: {e}", exc_info=True)
        return False

  async def _immediate_training(self, symbol: str, data: pd.DataFrame, features: pd.DataFrame) -> bool:
    """–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –º–µ—Ç–æ–∫"""
    try:
      logger.info(f"üß† {symbol}: –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

      # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–ï–¢–û–ö
      # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ—Ä–æ–≥–æ–≤
      atr_values = data['high'] - data['low']
      avg_volatility = atr_values.rolling(window=20).mean().iloc[-1] if len(atr_values) > 20 else atr_values.mean()
      current_price = data['close'].iloc[-1]
      volatility_ratio = avg_volatility / current_price if current_price > 0 else 0.02

      # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è 15M —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
      if volatility_ratio > 0.015:  # –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è 15M
        threshold = 0.4  # –°–Ω–∏–∂–µ–Ω–æ —Å 1.2% –¥–æ 0.4%
        future_bars = 4  # 4 –±–∞—Ä–∞ = 1 —á–∞—Å
        logger.info(f"üìä {symbol}: –≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {volatility_ratio:.4f}, –ø–æ—Ä–æ–≥={threshold}%")
      elif volatility_ratio > 0.008:  # –°—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è 15M
        threshold = 0.25  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.9% –¥–æ 0.25%
        future_bars = 4
        logger.info(f"üìä {symbol}: —Å—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {volatility_ratio:.4f}, –ø–æ—Ä–æ–≥={threshold}%")
      else:  # –ù–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è 15M
        threshold = 0.15  # –°–Ω–∏–∂–µ–Ω–æ —Å 0.6% –¥–æ 0.15%
        future_bars = 4  # –û—Å—Ç–∞–≤–ª—è–µ–º 4 –±–∞—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        logger.info(f"üìä {symbol}: –Ω–∏–∑–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å {volatility_ratio:.4f}, –ø–æ—Ä–æ–≥={threshold}%")

      # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ —Å –Ω–æ–≤–æ–π —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π
      labels = self._create_enhanced_labels(
        data,
        future_bars=future_bars,
        threshold_percent=threshold,
        symbol=symbol
      )

      # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
      common_index = features.index.intersection(labels.index)
      features_train = features.loc[common_index]
      labels_train = labels.loc[common_index]

      if len(features_train) < 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        logger.warning(f"‚ùå {symbol}: –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(features_train)}")
        return False

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
      class_counts = labels_train.value_counts()
      total_samples = len(labels_train)

      buy_ratio = class_counts.get(1, 0) / total_samples
      sell_ratio = class_counts.get(2, 0) / total_samples
      hold_ratio = class_counts.get(0, 0) / total_samples

      logger.info(
        f"üìà {symbol}: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ - BUY: {buy_ratio:.3f}, SELL: {sell_ratio:.3f}, HOLD: {hold_ratio:.3f}")

      # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ HOLD —Å–∏–≥–Ω–∞–ª–æ–≤, –ø—Ä–∏–º–µ–Ω—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É
      if hold_ratio > 0.8:
        logger.warning(f"‚ö†Ô∏è {symbol}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ HOLD ({hold_ratio:.3f}), –ø—Ä–∏–º–µ–Ω—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É")
        labels_train = self._balance_labels(labels_train, max_hold_ratio=0.7)

        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        class_counts = labels_train.value_counts()
        buy_ratio = class_counts.get(1, 0) / len(labels_train)
        sell_ratio = class_counts.get(2, 0) / len(labels_train)
        hold_ratio = class_counts.get(0, 0) / len(labels_train)
        logger.info(
          f"üìä {symbol}: –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ - BUY: {buy_ratio:.3f}, SELL: {sell_ratio:.3f}, HOLD: {hold_ratio:.3f}")

      # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
      classifier = EnhancedLorentzianClassifier(
        k_neighbors=self.neighbors_count,
        max_bars_back=self.max_bars_back,
        feature_count=self.feature_count,
        use_dynamic_exits=self.use_dynamic_exits,
        filters=self.filter_settings
      )

      classifier.fit(features_train, labels_train)

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ –∫–µ—à
      self.models_cache[symbol] = classifier

      # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –º–æ–¥–µ–ª–∏
      test_size = min(1000, len(features_train) // 2)

      # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞)
      train_features = features_train.iloc[:-test_size]
      train_labels_subset = labels_train.iloc[:-test_size]
      test_features = features_train.iloc[-test_size:]
      test_labels_subset = labels_train.iloc[-test_size:]

      # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
      classifier_test = EnhancedLorentzianClassifier(
        k_neighbors=self.neighbors_count,
        max_bars_back=self.max_bars_back,
        feature_count=self.feature_count,
        use_dynamic_exits=self.use_dynamic_exits,
        filters=self.filter_settings
      )

      classifier_test.fit(train_features, train_labels_subset)

      # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ out-of-sample –¥–∞–Ω–Ω—ã—Ö
      test_predictions = classifier_test.predict(test_features)
      test_labels_array = test_labels_subset.values

      # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
      accuracy = np.mean(test_predictions == test_labels_array)

      # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
      class_accuracies = {}
      for class_id in [0, 1, 2]:
        class_mask = test_labels_array == class_id
        if np.sum(class_mask) > 0:
          class_acc = np.mean(test_predictions[class_mask] == test_labels_array[class_mask])
          class_accuracies[class_id] = class_acc

      # –¢–æ—á–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ (BUY/SELL)
      trading_mask = (test_labels_array == 1) | (test_labels_array == 2)
      trading_accuracy = 0.0
      if np.sum(trading_mask) > 0:
        trading_accuracy = np.mean(test_predictions[trading_mask] == test_labels_array[trading_mask])

      logger.info(f"üéØ {symbol}: –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}, —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã={trading_accuracy:.3f}")
      logger.info(f"üìä {symbol}: —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º - HOLD={class_accuracies.get(0, 0):.3f}, "
                  f"BUY={class_accuracies.get(1, 0):.3f}, SELL={class_accuracies.get(2, 0):.3f}")

      # –ê–î–ê–ü–¢–ò–í–ù–´–ï –ö–†–ò–¢–ï–†–ò–ò –°–û–•–†–ê–ù–ï–ù–ò–Ø –¥–ª—è 15M
      # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏
      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏:
      # 1. –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å > 0.45 –ò–õ–ò
      # 2. –¢–æ—á–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ > 0.35 –ò –æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å > 0.35 –ò–õ–ò
      # 3. –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∞—è-—Ç–æ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ BUY –∏–ª–∏ SELL –∫–ª–∞—Å—Å–∞–º
      has_trading_accuracy = (class_accuracies.get(1, 0) > 0.2 or class_accuracies.get(2, 0) > 0.2)

      save_model = (
          accuracy > 0.45 or
          (trading_accuracy > 0.35 and accuracy > 0.35) or
          (has_trading_accuracy and accuracy > 0.30)
      )

      # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤ —Å —Ö–æ—Ä–æ—à–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–ª–∞—Å—Å–æ–≤
      good_distribution = (buy_ratio > 0.15 and sell_ratio > 0.15 and hold_ratio < 0.7)
      if good_distribution and accuracy > 0.30:
        save_model = True

      if save_model:
        self._save_model(symbol, classifier)
        logger.info(f"üíæ {symbol}: –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–æ–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}, —Ç–æ—Ä–≥–æ–≤–∞—è: {trading_accuracy:.3f})")
      else:
        logger.warning(
          f"‚ö†Ô∏è {symbol}: –º–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ - –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–æ–±—â–∞—è: {accuracy:.3f}, —Ç–æ—Ä–≥–æ–≤–∞—è: {trading_accuracy:.3f})")

      return True

    except Exception as e:
      logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è {symbol}: {e}", exc_info=True)
      return False

  def _balance_labels(self, labels: pd.Series, max_hold_ratio: float = 0.7) -> pd.Series:
    """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –¥–æ–ª–∏ HOLD"""

    class_counts = labels.value_counts()
    total_samples = len(labels)

    hold_count = class_counts.get(0, 0)
    buy_count = class_counts.get(1, 0)
    sell_count = class_counts.get(2, 0)

    current_hold_ratio = hold_count / total_samples

    # –î–ª—è 15M —Ä–∞–∑—Ä–µ—à–∞–µ–º –±–æ–ª—å—à–µ HOLD (–¥–æ 85% –≤–º–µ—Å—Ç–æ 70%)
    if current_hold_ratio <= 0.85:
      return labels  # –£–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 15M

    # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ HOLD –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    target_hold_count = int(total_samples * max_hold_ratio)
    excess_hold = hold_count - target_hold_count

    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã HOLD –º–µ—Ç–æ–∫
    hold_indices = labels[labels == 0].index.tolist()

    # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º HOLD –º–µ—Ç–∫–∏ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    import random
    random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    convert_indices = random.sample(hold_indices, min(excess_hold, len(hold_indices)))

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –º–µ—Ç–∫–∏
    balanced_labels = labels.copy()

    for idx in convert_indices:
      # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ BUY –∏–ª–∏ SELL –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
      if random.random() < 0.5:
        balanced_labels.loc[idx] = 1  # BUY
      else:
        balanced_labels.loc[idx] = 2  # SELL

    return balanced_labels

  def _create_enhanced_labels(self, data: pd.DataFrame, future_bars: int = 4,
                              threshold_percent: float = 0.85, symbol: str = "") -> pd.Series:
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏"""

    df = data.copy()

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    df['returns'] = df['close'].pct_change()
    df['volatility'] = df['returns'].rolling(window=20).std()
    df['volume_ratio'] = df['volume'] / df['volume'].rolling(window=20).mean()

    # ATR –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df['atr'] = true_range.rolling(window=14).mean()

    labels = []
    buy_count = 0
    sell_count = 0

    for i in range(len(df)):
      if i + future_bars >= len(df):
        labels.append(0)  # HOLD –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –±–∞—Ä–æ–≤
        continue

      current_price = df.iloc[i]['close']
      current_atr = df.iloc[i]['atr'] if pd.notna(df.iloc[i]['atr']) else current_price * 0.02
      current_vol_ratio = df.iloc[i]['volume_ratio'] if pd.notna(df.iloc[i]['volume_ratio']) else 1.0


      # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è 15M)
      atr_factor = current_atr / current_price

      # –î–ª—è 15M –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
      # ATR –æ–±—ã—á–Ω–æ 0.001-0.003 –¥–ª—è –∫—Ä–∏–ø—Ç–æ –Ω–∞ 15M, —á—Ç–æ –¥–∞–µ—Ç 0.1-0.3%
      # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 0.5-1.0 –≤–º–µ—Å—Ç–æ 2.0 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
      dynamic_threshold = max(threshold_percent / 100, atr_factor * 0.7)

      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è 15M
      dynamic_threshold = min(dynamic_threshold, 0.004)  # –ú–∞–∫—Å–∏–º—É–º 0.4%
      dynamic_threshold = max(dynamic_threshold, 0.001)  # –ú–∏–Ω–∏–º—É–º 0.1%

      # –ê–Ω–∞–ª–∏–∑ –±—É–¥—É—â–∏—Ö —Ü–µ–Ω
      future_slice = df.iloc[i + 1:i + future_bars + 1]
      future_highs = future_slice['high']
      future_lows = future_slice['low']
      future_closes = future_slice['close']

      max_high = future_highs.max()
      min_low = future_lows.min()
      final_close = future_closes.iloc[-1]

      # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è
      max_upside = (max_high - current_price) / current_price
      max_downside = (current_price - min_low) / current_price

      if i % 500 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é 500-—é —Å–≤–µ—á—É
        logger.debug(f"{symbol} [{i}]: upside={max_upside:.4f}, downside={max_downside:.4f}, "
                     f"threshold={dynamic_threshold:.4f}, atr_factor={atr_factor:.4f}")
      final_return = (final_close - current_price) / current_price

      # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–≤–∏–∂–µ–Ω–∏—è
      price_momentum = final_return / dynamic_threshold if dynamic_threshold > 0 else 0
      volume_confirmation = current_vol_ratio > 1.1

      # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è BUY
      strong_buy = (
          max_upside > dynamic_threshold * 1.5 and
          final_return > dynamic_threshold * 0.7 and
          max_upside > max_downside * 1.3 and
          abs(price_momentum) > 1.0
      )

      # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è SELL
      strong_sell = (
          max_downside > dynamic_threshold * 1.5 and
          final_return < -dynamic_threshold * 0.7 and
          max_downside > max_upside * 1.3 and
          abs(price_momentum) > 1.0
      )

      # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
      if strong_buy and (not volume_confirmation or buy_count < sell_count + 50):
        labels.append(1)  # BUY
        buy_count += 1
      elif strong_sell and (not volume_confirmation or sell_count < buy_count + 50):
        labels.append(2)  # SELL
        sell_count += 1
      else:
        labels.append(0)  # HOLD

    labels_series = pd.Series(labels, index=df.index)

    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    class_counts = labels_series.value_counts()
    total = len(labels_series)
    logger.info(
      f"üè∑Ô∏è {symbol}: –º–µ—Ç–∫–∏ —Å–æ–∑–¥–∞–Ω—ã - BUY={class_counts.get(1, 0)} ({class_counts.get(1, 0) / total * 100:.1f}%), "
      f"SELL={class_counts.get(2, 0)} ({class_counts.get(2, 0) / total * 100:.1f}%), "
      f"HOLD={class_counts.get(0, 0)} ({class_counts.get(0, 0) / total * 100:.1f}%)")

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    buy_final = labels_series.eq(1).sum()
    sell_final = labels_series.eq(2).sum()
    hold_final = labels_series.eq(0).sum()
    total = len(labels_series)

    logger.info(f"üè∑Ô∏è {symbol}: –º–µ—Ç–∫–∏ —Å–æ–∑–¥–∞–Ω—ã - BUY={buy_final} ({buy_final / total * 100:.1f}%), "
                f"SELL={sell_final} ({sell_final / total * 100:.1f}%), "
                f"HOLD={hold_final} ({hold_final / total * 100:.1f}%)")

    if (buy_final + sell_final) / total < 0.03:
      logger.warning(f"‚ö†Ô∏è {symbol}: –û—á–µ–Ω—å –º–∞–ª–æ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤! "
                     f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –µ—â–µ —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥–∏")



    return labels_series

  def _apply_filters(self, data: pd.DataFrame, symbol: str = None) -> Dict[str, bool]:
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ + WaveTrend 3D
    """
    results = {}

    # Volatility Filter
    if self.filter_settings['use_volatility_filter']:
      volatility_pass = self.filters.volatility_filter(data)
      results['volatility'] = bool(volatility_pass.iloc[-1]) if not volatility_pass.empty else True
    else:
      results['volatility'] = True

    # Regime Filter
    if self.filter_settings['use_regime_filter']:
      regime_pass = self.filters.regime_filter(data, self.filter_settings['regime_threshold'])
      results['regime'] = bool(regime_pass.iloc[-1]) if not regime_pass.empty else True
    else:
      results['regime'] = True

    # ADX Filter
    if self.filter_settings['use_adx_filter']:
      adx_pass = self.filters.adx_filter(data, self.filter_settings['adx_threshold'])
      results['adx'] = bool(adx_pass.iloc[-1]) if not adx_pass.empty else True
    else:
      results['adx'] = True

    # WaveTrend 3D Filter
    if self.use_wavetrend_3d and self.wavetrend_3d and symbol:
      wavetrend_signal = self.wavetrend_3d.get_signal_for_lorentzian(data, symbol)
      if wavetrend_signal:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        results['wavetrend_3d'] = True
        results['wavetrend_direction'] = wavetrend_signal['direction']
        results['wavetrend_confidence'] = wavetrend_signal['confidence']
      else:
        results['wavetrend_3d'] = True  # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–∞
        results['wavetrend_direction'] = 0
        results['wavetrend_confidence'] = 0.5
    else:
      results['wavetrend_3d'] = True

    return results

  def update_model(self, symbol: str, new_data: pd.DataFrame, actual_outcome: int):
    """
    –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

    Args:
        symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
        new_data: DataFrame —Å –Ω–æ–≤—ã–º –±–∞—Ä–æ–º (OHLCV)
        actual_outcome: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (-1=SELL, 0=HOLD, 1=BUY)
    """
    if symbol not in self.models_cache:
      logger.warning(f"–ú–æ–¥–µ–ª—å –¥–ª—è {symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
      return

    try:
      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –±–∞—Ä–∞
      new_features = self.indicators.calculate_features(
        new_data,
        self.feature_configs['f1'],
        self.feature_configs['f2'],
        self.feature_configs['f3'],
        self.feature_configs['f4'],
        self.feature_configs['f5']
      )

      if new_features.empty:
        return

      # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
      feature_vector = new_features.iloc[-1].values

      # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å
      classifier = self.models_cache[symbol]
      classifier.incremental_update(feature_vector, actual_outcome, symbol)

      # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à –¥–∞–Ω–Ω—ã—Ö
      if symbol not in self.training_data_cache:
        self.training_data_cache[symbol] = {
          'features': [],
          'labels': [],
          'update_count': 0,
          'last_accuracy': 0.0
        }

      cache = self.training_data_cache[symbol]
      cache['features'].append(feature_vector)
      cache['labels'].append(actual_outcome)
      cache['update_count'] += 1

      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–µ—à–∞
      if len(cache['features']) > self.max_bars_back:
        cache['features'].pop(0)
        cache['labels'].pop(0)

      # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
      if cache['update_count'] % 20 == 0:
        self._evaluate_model_performance(symbol)

      if cache['update_count'] % 50 == 0 and self.settings.get('save_models', True):
        self._save_model(symbol, classifier)
        logger.info(f"–ú–æ–¥–µ–ª—å {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ—Å–ª–µ {cache['update_count']} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π")

      # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ 100 –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
      if cache['update_count'] % 2 == 0 and self.settings.get('save_models', True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"ml_models/lorentzian_{symbol}_{timestamp}.pkl"
        classifier.save_model(model_path)
        logger.info(f"–ú–æ–¥–µ–ª—å {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ—Å–ª–µ {cache['update_count']} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π")

      logger.debug(f"–ú–æ–¥–µ–ª—å {symbol} –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –í—Å–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {cache['update_count']}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è {symbol}: {e}", exc_info=True)

  def _evaluate_model_performance(self, symbol: str):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
    try:
      cache = self.training_data_cache.get(symbol)
      if not cache or len(cache['features']) < 50:
        return

      classifier = self.models_cache.get(symbol)
      if not classifier:
        return

      # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
      test_features = np.array(cache['features'][-50:])
      test_labels = np.array(cache['labels'][-50:])

      # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      feature_names = [f'f{i + 1}' for i in range(self.feature_count)]
      test_df = pd.DataFrame(test_features, columns=feature_names)

      # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
      predictions = classifier.predict(test_df)

      # –°—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
      accuracy = np.mean(predictions == test_labels)

      # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
      improvement = accuracy - cache['last_accuracy']
      cache['last_accuracy'] = accuracy

      logger.info(f"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å {symbol}: —Ç–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}, "
                  f"–∏–∑–º–µ–Ω–µ–Ω–∏–µ={improvement:+.3f}")

      # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–ª—å–Ω–æ —É–ø–∞–ª–∞, –º–æ–∂–µ–º –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
      if improvement < -0.1 and len(cache['features']) >= self.min_history_size:
        logger.warning(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {symbol}. "
                       f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {symbol}: {e}")

  def _update_nearest_neighbors(self, symbol: str, new_feature_vector: np.ndarray, new_label: int):
    """
    –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    —ç–º—É–ª–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≤ TradingView
    """
    if symbol not in self.models_cache:
      return

    classifier = self.models_cache[symbol]

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Ç–æ—á–∫—É –≤ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å
    # –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã EnhancedLorentzianClassifier

    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ - –ø–æ–º–µ—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    if not hasattr(classifier, 'pending_updates'):
      classifier.pending_updates = []

    classifier.pending_updates.append((new_feature_vector, new_label))

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º predict
    if len(classifier.pending_updates) > 10:
      logger.debug(f"–ù–∞–∫–æ–ø–ª–µ–Ω–æ {len(classifier.pending_updates)} –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –¥–ª—è {symbol}")

  async def process_trade_feedback(self, symbol: str, trade_result: Dict) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–¥–µ–ª–∫–∏ –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    try:
      # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
      profit_loss = trade_result.get('profit_loss', 0)
      if profit_loss > 0:
        actual_outcome = 1  # –°–∏–≥–Ω–∞–ª –±—ã–ª –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º
      elif profit_loss < 0:
        actual_outcome = -1  # –°–∏–≥–Ω–∞–ª –±—ã–ª –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º
      else:
        actual_outcome = 0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

      # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
      if hasattr(self, 'data_fetcher') and self.data_fetcher:
        recent_data = await self.data_fetcher.get_historical_candles(
          symbol,
          Timeframe.FIFTEEN_MINUTES,
          limit=50
        )

        if not recent_data.empty:
          self.update_model(symbol, recent_data, actual_outcome)
          logger.info(f"‚úÖ Lorentzian –º–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –¥–ª—è {symbol}, —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {actual_outcome}")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –æ—Ç–∑—ã–≤–∞ –≤ Lorentzian: {e}")

  def _accumulate_training_data(self, symbol: str, data: pd.DataFrame, features: pd.DataFrame) -> bool:
    """
    –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø - –±–µ—Ä–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É

    Returns:
        True –µ—Å–ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    try:
      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
      if symbol not in self.training_history:
        self.training_history[symbol] = {
          'combined_data': pd.DataFrame(),
          'combined_features': pd.DataFrame(),
          'accumulated_count': 0,
          'last_update': datetime.now(),
          'is_ready': False
        }
        logger.info(f"üÜï {symbol}: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

      history = self.training_history[symbol]

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
      if data.empty or features.empty:
        logger.warning(f"‚ùå {symbol}: –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è")
        return False

      # –ë–ï–†–ï–ú –í–°–ï –î–û–°–¢–£–ü–ù–´–ï –î–ê–ù–ù–´–ï –°–†–ê–ó–£
      logger.info(f"üì• {symbol}: –ø–æ–ª—É—á–µ–Ω–æ {len(data)} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö, {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
      min_len = min(len(data), len(features))
      if min_len == 0:
        logger.warning(f"‚ùå {symbol}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return False

      # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É
      aligned_data = data.head(min_len).copy()
      aligned_features = features.head(min_len).copy()

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑ - –±–µ—Ä–µ–º —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
      history['combined_data'] = aligned_data
      history['combined_features'] = aligned_features
      history['accumulated_count'] = len(aligned_data)
      history['last_update'] = datetime.now()

      logger.info(f"‚úÖ {symbol}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {history['accumulated_count']} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é
      is_ready = history['accumulated_count'] >= self.min_initial_training_size
      history['is_ready'] = is_ready

      if is_ready:
        logger.info(
          f"üéØ {symbol}: ‚úÖ –î–û–°–¢–ê–¢–û–ß–ù–û –î–ê–ù–ù–´–• ({history['accumulated_count']}>={self.min_initial_training_size})! "
          f"–ì–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é!")
      else:
        needed = self.min_initial_training_size - history['accumulated_count']
        logger.info(f"‚è≥ {symbol}: –Ω–∞–∫–æ–ø–ª–µ–Ω–æ {history['accumulated_count']}/{self.min_initial_training_size}, "
                    f"–Ω—É–∂–Ω–æ –µ—â–µ {needed} —Ç–æ—á–µ–∫")

      return is_ready

    except Exception as e:
      logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}", exc_info=True)
      return False

  def log_accumulation_summary(self):
    """–õ–æ–≥–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    try:
      if not self.training_history:
        logger.info("üìä –°–í–û–î–ö–ê –ù–ê–ö–û–ü–õ–ï–ù–ò–Ø: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
        return

      ready_count = 0
      total_symbols = len(self.training_history)

      logger.info("üìä –°–í–û–î–ö–ê –ù–ê–ö–û–ü–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•:")
      logger.info("=" * 60)

      for symbol, history in self.training_history.items():
        progress_pct = (history['accumulated_count'] / self.min_initial_training_size) * 100
        status = "‚úÖ –ì–û–¢–û–í" if history['accumulated_count'] >= self.min_initial_training_size else "‚è≥ –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ"

        if status == "‚úÖ –ì–û–¢–û–í":
          ready_count += 1

        logger.info(f"{symbol:12} | {history['accumulated_count']:5}/{self.min_initial_training_size} "
                    f"({progress_pct:5.1f}%) | {status} | –°–µ—Å—Å–∏–π: {history['session_count']:3}")

      logger.info("=" * 60)
      logger.info(f"–ò–¢–û–ì–û: {ready_count}/{total_symbols} —Å–∏–º–≤–æ–ª–æ–≤ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é")

    except Exception as e:
      logger.error(f"–û—à–∏–±–∫–∞ –≤ —Å–≤–æ–¥–∫–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è: {e}")

  def _get_accumulated_data(self, symbol: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
    –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø

    Returns:
        Tuple of (combined_data, combined_features)
    """
    try:
      if symbol not in self.training_history:
        logger.warning(f"‚ùå {symbol}: –Ω–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        return pd.DataFrame(), pd.DataFrame()

      history = self.training_history[symbol]

      combined_data = history.get('combined_data', pd.DataFrame())
      combined_features = history.get('combined_features', pd.DataFrame())

      if combined_data.empty or combined_features.empty:
        logger.warning(f"‚ùå {symbol}: –ø—É—Å—Ç—ã–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        return pd.DataFrame(), pd.DataFrame()

      logger.info(f"üì§ {symbol}: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º {len(combined_data)} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö, "
                  f"{len(combined_features)} —Ç–æ—á–µ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

      return combined_data, combined_features

    except Exception as e:
      logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}", exc_info=True)
      return pd.DataFrame(), pd.DataFrame()

  def get_accumulation_status(self, symbol: str = None) -> Dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    if symbol:
      # –°—Ç–∞—Ç—É—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
      if symbol in self.training_history:
        history = self.training_history[symbol]
        return {
          'symbol': symbol,
          'accumulated_count': history['accumulated_count'],
          'required_count': self.min_initial_training_size,
          'progress_pct': (history['accumulated_count'] / self.min_initial_training_size) * 100,
          'ready_for_training': history['accumulated_count'] >= self.min_initial_training_size,
          'last_update': history['last_update']
        }
      else:
        return {
          'symbol': symbol,
          'accumulated_count': 0,
          'required_count': self.min_initial_training_size,
          'progress_pct': 0.0,
          'ready_for_training': False,
          'last_update': None
        }
    else:
      # –°—Ç–∞—Ç—É—Å –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
      status = {}
      for sym, history in self.training_history.items():
        status[sym] = {
          'accumulated_count': history['accumulated_count'],
          'required_count': self.min_initial_training_size,
          'progress_pct': (history['accumulated_count'] / self.min_initial_training_size) * 100,
          'ready_for_training': history['accumulated_count'] >= self.min_initial_training_size,
          'last_update': history['last_update']
        }
      return status

  async def preload_training_data(self, symbol: str, force_reload: bool = False):
    """
    –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏.
    """
    target_count = self.min_initial_training_size
    timeframe = Timeframe.FIFTEEN_MINUTES  # –£–∫–∞–∂–∏—Ç–µ –Ω—É–∂–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–∞ –ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞
    if not force_reload and symbol in self.training_history:
      current_count = self.training_history[symbol].get('accumulated_count', 0)
      if current_count >= target_count:
        logger.info(f"‚úÖ {symbol}: –î–∞–Ω–Ω—ã–µ —É–∂–µ –Ω–∞–∫–æ–ø–ª–µ–Ω—ã ({current_count} —Ç–æ—á–µ–∫), –ø—Ä–æ–ø—É—Å–∫.")
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        # if symbol not in self.models_cache or not self.models_cache[symbol].is_fitted:
        #   logger.info(f"–ú–æ–¥–µ–ª—å –¥–ª—è {symbol} –Ω–µ –æ–±—É—á–µ–Ω–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
        #   # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
        #   try:
        #     data, features = self._get_accumulated_data(symbol)
        #     if not data.empty and not features.empty:
        #       await self._immediate_training(symbol, data, features)
        #   except Exception as e:
        #     logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
        # return True
        if symbol not in self.models_cache or not self.models_cache[symbol].is_fitted:
          logger.info(f"‚è≥ {symbol}: –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É...")
          # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –≤ —Ñ–æ–Ω–µ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞
          asyncio.create_task(self.preload_training_data(symbol))
          return None

    if not self.data_fetcher:
      logger.error(f"‚ùå DataFetcher –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ {symbol}")
      return False

    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è {symbol}. –¶–µ–ª—å: {target_count} —Å–≤–µ—á–µ–π.")

    all_candles = pd.DataFrame()
    api_limit_per_request = 1000  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ª–∏–º–∏—Ç API –±–∏—Ä–∂
    end_time_ms = None  # –î–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ø—Ä–æ—à–ª–æ–µ

    max_requests = 10  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    for i in range(max_requests):
      if len(all_candles) >= target_count:
        logger.info(f"üéØ {symbol}: –¶–µ–ª—å –≤ {target_count} —Å–≤–µ—á–µ–π –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞. –°–æ–±—Ä–∞–Ω–æ: {len(all_candles)}.")
        break

      logger.info(f"‚è≥ {symbol}: –ó–∞–ø—Ä–æ—Å #{i + 1}. –°–æ–±—Ä–∞–Ω–æ {len(all_candles)}/{target_count}...")

      # –í–ê–ñ–ù–û: –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤–∞—à data_fetcher –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å 'params' –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ 'endTime' –≤ API
      # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ Binance)
      # –°–ª–æ–≤–∞—Ä—å —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
      request_kwargs = {
        'symbol': symbol,
        'timeframe': timeframe,
        'limit': api_limit_per_request
      }
      # –î–æ–±–∞–≤–ª—è–µ–º 'end' —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, —á—Ç–æ–±—ã –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é
      if end_time_ms:
        request_kwargs['end'] = end_time_ms

      try:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –í–´–ó–û–í: –ø–µ—Ä–µ–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ **
        chunk = await self.data_fetcher.get_historical_candles(**request_kwargs)

      except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ API –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
        await asyncio.sleep(5)
        continue

      if chunk is None or chunk.empty:
        logger.warning(f"‚ö†Ô∏è {symbol}: –ë–æ–ª—å—à–µ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –°–æ–±—Ä–∞–Ω–æ {len(all_candles)}.")
        break
      # --- –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ò –ë–û–õ–ï–ï –ù–ê–î–ï–ñ–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
      # –Ø–≤–Ω–æ –±–µ—Ä–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —á–∞–Ω–∫–∞
      oldest_timestamp = chunk.index.min()
      end_time_ms = int(oldest_timestamp.timestamp() * 1000) - 1
      # -------------------------------------------

      # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–æ –æ–±—â–µ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
      all_candles = pd.concat([chunk, all_candles])

      # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º `endTime` –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ 1–º—Å —Ä–∞–Ω—å—à–µ —Å–∞–º–æ–π —Å—Ç–∞—Ä–æ–π —Å–≤–µ—á–∏ –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö
      # –î–∞–Ω–Ω—ã–µ –æ–±—ã—á–Ω–æ –ø—Ä–∏—Ö–æ–¥—è—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º
      end_time_ms = int(chunk.index[0].timestamp() * 1000) - 1

      # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–æ –æ–±—â–µ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
      all_candles = pd.concat([chunk, all_candles])

      # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–∞ —Å–ª—É—á–∞–π –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
      all_candles = all_candles[~all_candles.index.duplicated(keep='first')]

      # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
      await asyncio.sleep(2)

    if all_candles.empty:
      logger.error(f"‚ùå {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ.")
      return False

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –¥–∞–Ω–Ω—ã–µ —à–ª–∏ –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
    all_candles.sort_index(inplace=True)
    logger.info(f"üßÆ {symbol}: –î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã ({len(all_candles)} —Å–≤–µ—á–µ–π). –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    features = self.indicators.calculate_features(
      all_candles,
      self.feature_configs['f1'], self.feature_configs['f2'], self.feature_configs['f3'],
      self.feature_configs['f4'], self.feature_configs['f5']
    )

    if features.empty:
      logger.error(f"‚ùå {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏.")
      return False

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ä–∞–∑—É –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    self._accumulate_training_data(symbol, all_candles, features)
    success = await self._immediate_training(symbol, all_candles, features)

    if success:
      logger.info(f"‚úÖ‚úÖ‚úÖ {symbol}: –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    else:
      logger.warning(f"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è {symbol}: –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å.")

    return success

  async def preload_multiple_symbols(self, symbols: List[str], max_concurrent: int = 3):
    """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    import asyncio

    async def preload_single(symbol):
      try:
        return await self.preload_training_data(symbol)
      except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ {symbol}: {e}")
        return False

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
    results = {}
    for i in range(0, len(symbols), max_concurrent):
      batch = symbols[i:i + max_concurrent]
      logger.info(f"üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ {i // max_concurrent + 1}: {batch}")

      batch_tasks = [preload_single(symbol) for symbol in batch]
      batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

      for symbol, result in zip(batch, batch_results):
        results[symbol] = result

      # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
      if i + max_concurrent < len(symbols):
        await asyncio.sleep(1)

    successful = sum(1 for r in results.values() if r is True)
    logger.info(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful}/{len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤ –≥–æ—Ç–æ–≤—ã")

    return results